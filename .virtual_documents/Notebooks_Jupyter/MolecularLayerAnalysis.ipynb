





############################################################################################################################ 
# Get the latest CREST files for each ID within the target folder (dirname)

from pathlib import Path
import json
from sqlite3 import connect as sqlite3_connect
from sqlite3 import DatabaseError
from google.cloud import bigquery
import cloudvolume
from igraph import Graph as ig_Graph
from igraph import plot as ig_plot
from scipy.optimize import curve_fit
from scipy.spatial.distance import cdist
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import Normalizer
from scipy.stats import zscore
from random import choice as random_choice
from itertools import combinations
from numpy import array, unravel_index, argmin, mean
import random
import numpy as np
from copy import deepcopy
import itertools
from datetime import datetime
from time import time
import neuroglancer
from webbrowser import open as wb_open
from webbrowser import open_new as wb_open_new
import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt
from tqdm import tqdm


import sys
sys.path.append('/Users/kperks/Documents/ell-connectome/efish_em/efish_em')

# from eCREST_cli_beta import ecrest, import_settings
from eCREST_cli import ecrest
import AnalysisCode as efish 





path_to_settings_json = '/Users/kperks/Documents/ell-connectome/eCREST-local-files/settings_dict.json'
settings_dict = efish.import_settings(path_to_settings_json)

vx_sizes = [16,16,30]





filepath = Path('/Users/kperks/Documents/ell-connectome/efish_em')
filename = 'efish_em.mplstyle'
plt.style.use(filepath/filename)


cell_colors = efish.color_palette('cell')
cell_colors['mli'] = ['#aaaaaa']
structure_colors = efish.color_palette('structure')





neuroglancer_path = Path(settings_dict['save_dir']).parent.parent / 'blender/soma_locations/layer-molecular_annotation.json'

with open(Path(neuroglancer_path), 'r') as myfile: # 'p' is the dirpath and 'f' is the filename from the created 'd' dictionary
    neuroglancer_data = json.load(myfile)

set([item['name'] for item in neuroglancer_data['layers'] if item['type']=='annotation'])

nl_ = 'molecular'
neuroglancer_layer = next((item for item in neuroglancer_data['layers'] if item["name"] == nl_), None)
voxel_sizes = [16,16,30]

vertices = [[p['point'][i]*voxel_sizes[i] for i in range(3)] for p in neuroglancer_layer['annotations']] #[p['point'] for p in neuroglancer_layer['annotations']]#

x_pts = [p[0] for p in vertices]
y_pts = [p[1] for p in vertices]
z_pts = [p[2] for p in vertices]

# Perform curve fitting
popt, pcov = curve_fit(efish.func_planar_curve, (x_pts, z_pts), y_pts)

# Print optimized parameters
print(popt)






dirpath = Path(settings_dict['save_dir'])

nodefiles = efish.get_cell_filepaths(dirpath)








df_type = pd.read_csv(dirpath / 'metadata/df_type_auto_typed.csv')








df_syn = pd.read_csv(dirpath / 'graphs/df_postsyn.csv')
syn = 'post-synaptic'


len(df_syn)


y_adj_col = []
for i,r in df_syn.iterrows():
    yoffset = efish.func_planar_curve((r['x'], r['z']), *popt)
    y_adj = (r['y'] - yoffset)
    y_adj_col.append(y_adj)

df_syn.loc[:,'y_adj']=y_adj_col

for v in ['x','y','z','y_adj']:
    df_syn[v] = df_syn[v]/1000
df_syn['y_adj'] = df_syn['y_adj']*-1    


# pf_df = deepcopy(df_syn[(df_syn['pre_type'].isin(['pf']))&(df_syn['post_type'].isin(['mg1','mg2','lg','lf']))])
# pf_df['x']=pf_df['x'].apply(lambda x: x/16)
# pf_df['y']=pf_df['y'].apply(lambda x: x/16)
# pf_df['z']=pf_df['z'].apply(lambda x: x/30)

# pf_df.to_csv(Path('/Users/kperks/Downloads/pf_synapses.csv'))





for i,r in df_syn.iterrows():
    try:
        df_syn.loc[i,'pre_type'] =df_type[df_type['id'].isin([r['pre']])].cell_type.values[0]
        df_syn.loc[i,'post_type']=df_type[df_type['id'].isin([r['post']])].cell_type.values[0]
    except:
        print(r['pre'],r['post'])
        continue

df_syn.loc[:,'post_type'] = [t.lower() for t in df_syn['post_type']]
df_syn.loc[:,'pre_type'] = [t.lower() for t in df_syn['pre_type']]


mask = df_syn['pre_type'].isin(['pf'])
df_syn = df_syn[mask]





syn = 'post-synaptic'
source = 'pre'

check_types = ['pf']#set(df_syn['pre_type'].unique()) | set(df_syn['post_type'].unique())

df_progress = efish.check_annot_reconstruction_completeness(df_syn, nodefiles, df_type, syn, source, check_types)


df_progress[(df_progress['cell_type'].isin(['pf']))].sort_values('todo')# & (df_progress['todo']>0) #grc','sgx2','sg2','mg2


mask = df_progress['completed']<0.95
pre_incomplete = np.asarray([int(c) for c in df_progress[mask]['id'].unique()])








df_syn = df_syn[~df_syn['pre'].isin(pre_incomplete)]





df_edges=df_syn[['pre','post','pre_type','post_type']].value_counts().reset_index(name='weight')





df_edges[df_edges['pre'].isin([368151283])]['post'].unique()


df_filtered = df_edges[(df_edges['post_type'].isin(['mg1','mg2']))].groupby(['pre','post_type']).sum(numeric_only=True)


df_filtered


# has_both_types = (
#     df_filtered.index
#     .to_frame(index=False)
#     .groupby('pre')['post_type']
#     .apply(lambda x: {'mg1', 'mg2'}.issubset(set(x)))
# )

# # Step 2: Filter rows where 'pre' IDs have both 'mg1' and 'mg2'
# result = df_filtered[df_filtered.index.get_level_values('pre').isin(has_both_types[has_both_types].index)]

# print(result)


# Step 1: Identify 'pre' IDs associated with both 'mg1' and 'mg2'
pre_with_both = (
    df_filtered.reset_index()
    .groupby('pre')['post_type']
    .apply(lambda x: {'mg1', 'mg2'}.issubset(set(x)))
)

# Step 2: Filter rows where 'pre' IDs have both 'mg1' and 'mg2'
result = df_filtered[df_filtered.index.get_level_values('pre').isin(pre_with_both[pre_with_both].index)]

result.to_csv('/Users/kperks/Downloads/result.csv')


result.head(100)





hfig,ax = plt.subplots(1,figsize=(4,3))
sns.histplot(data=df_edges[df_edges['post_type'].isin(['lg','lf','mg1','mg2','sg1','sg2','smpl'])],x='weight',color='black',ax=ax,bins=np.arange(0.5,16.5))
ax.set_xticks(np.arange(0,17,2));





df_syn['depth'] = pd.cut(df_syn['y_adj'], bins=np.arange(0,350,50))


df_syn.head()


mask = df_syn['post_type'].isin(['sg1','sg2','mg1','mg2','lg','lf','smpl','mli','tsd','h'])# df_edges['pre'].isin([295969348,295969442,295969134,295969355,295968777,282228761,283375247,283391297,283390956,282230475,268614458,268614383,273086215,187230424,290552453,27220895,31694533,102463116,188296613,15401313,17877032,187151336,117041378,122039969,36165549]) &  


df_grouped = df_syn[mask][['pre','post_type','depth']].groupby(['depth','post_type'],observed=False).count().reset_index()


df_grouped.head()#[df_grouped['weight']>0]


df_grouped.loc[df_grouped['post_type'].isin(['sg1','sg2']),'class'] = 'sg'
df_grouped.loc[df_grouped['post_type'].isin(['mg1','mg2']),'class'] = 'mg'
df_grouped.loc[df_grouped['post_type'].isin(['lf','lg']),'class'] = 'output'
df_grouped.loc[df_grouped['post_type'].isin(['smpl','mli','tsd','h']),'class'] = 'smooth'


df_grouped['weight_norm'] = df_grouped.groupby('depth',observed=False)['pre'].transform(lambda x: (x / x.sum()) )

df_grouped['weight_all'] = df_grouped.groupby('depth',observed=False)['pre'].transform(lambda x: (x.sum()) )



# Create a seaborn plot
hfig,ax = plt.subplots(1,figsize=(2,5))
sns.pointplot(data=df_grouped,y='depth', x='weight_norm',hue = 'class',linewidth=1)#,order = ['lg','mg1','sg1','lf','mg2','sg2'])
ax.invert_yaxis()
plt.savefig('/Users/kperks/Downloads/PFsynapse_locations_binned_normalized.svg',transparent=True) 


# sns.pointplot(data=df_grouped,x='depth', y='weight_all',color='black')
hfig,ax = plt.subplots(1,figsize=(2,5))
sns.pointplot(data=df_grouped,y='depth', x='weight_all',color='black',linewidth=1)#,order = ['lg','mg1','sg1','lf','mg2','sg2'])
ax.invert_yaxis()
plt.savefig('/Users/kperks/Downloads/PFsynapse_locations_binned_TOTAL.svg',transparent=True) 






g = sns.jointplot(data=df_syn[df_syn['post_type'].isin(['mg2','mg1','sg2','sg1','lg','lf','smpl','mli','tsd','h','grc'])], #['aff','sg2','mg2','sg1','mg1','grc-d','grc-s']
                  x="x", y="y_adj", hue="post_type", height=10,
                  alpha = 0.5, edgecolor='black',palette = syn_colors,
                 marginal_kws=dict(common_norm=True,clip_on=0, fill=False), space=0)
# sns.scatterplot(x=[0],y=[0],color='black',s=100);

sns_joint_equal_axes(g);
sns.move_legend(plt.gca(), "upper right", bbox_to_anchor=(1.4, 1));

# g = sns.jointplot(data=df_syn[df_syn['pre_type'].isin(['mg2','mg1'])], #['aff','sg2','mg2','sg1','mg1','grc-d','grc-s']
#                   x="x", y="z", hue="pre_type", height=10,
#                   alpha = 0.5, edgecolor='black',palette = syn_colors,
#                  marginal_kws=dict(common_norm=True,clip_on=0, fill=False), space=0)
# # sns.scatterplot(x=[0],y=[0],color='black',s=100);

# sns_joint_equal_axes(g);
# sns.move_legend(plt.gca(), "upper right", bbox_to_anchor=(1.4, 1));





'''
conda install conda-forge::google-cloud-sdk

Then, launch jupyter lab 

In a code cell, run bash command <!gcloud auth login > (https://cloud.google.com/sdk/gcloud/reference/auth/login)
    a browser tab should open up

RESULT:
You are now logged in as [kperky@gmail.com].
Your current project is [lcht-goog-connectomics].  You can change this setting by running:
  $ gcloud config set project PROJECT_ID
'''
!gcloud auth login 



bigquery_client = bigquery.Client(project='lcht-goog-connectomics')


jsonpath = '/Users/kperks/Library/CloudStorage/GoogleDrive-sawtelllab@gmail.com/My Drive/ELL_connectome/VAST/VAST_consolidate_reconstructed/json_states/GCA_spines_LF.json'
path2skeletondir = '/Users/kperks/Library/CloudStorage/GoogleDrive-sawtelllab@gmail.com/My Drive/ELL_connectome/VAST/VAST_consolidate_reconstructed/skeletons'
skeleton_folder = Path(path2skeletondir) / 'Fig1_gca_spines_lf'


with open(jsonpath) as f:# = open(path2statesdir+'/'+cellid_filename[ind])
    data = json.load(f)


for layer in data.get('layers'):
    base_seg_list = layer['segments']
    lname = layer['name']
    base_seg_str = ', '.join(str(x) for x in base_seg_list)

    print(f'working on {lname}')
    df = pd.DataFrame()
    
    QUERY = """
    SELECT
        cast(objects.id as INT64) as seg_id,
        sample_voxel.x as x,
        sample_voxel.y as y,
        sample_voxel.z as z,
    FROM
        `lcht-goog-connectomics.ell_roi450um_seg32fb16fb_220930.objinfo` as objects
    WHERE objects.id in {}
    """.format('('+base_seg_str+')')
    
    df = bigquery_client.query(QUERY).to_dataframe()
    df = df.drop_duplicates(subset=['seg_id'])
    
    # saveing information
    # timestr = time.strftime("%Y%m%d-%H%M%S")
    savename_and_path = path2skeletondir / skeleton_folder / f'{lname}_skeleton.csv';
    df.to_csv(savename_and_path, index=False)








df_syn.loc[df_syn['post_type'].isin(['aff']),'post_class'] = 'eaf'
df_syn.loc[df_syn['post_type'].isin(['sg1','sg2']),'post_class'] = 'sg'
df_syn.loc[df_syn['post_type'].isin(['mg1','mg2']),'post_class'] = 'mg'
df_syn.loc[df_syn['post_type'].isin(['lg','lf']),'post_class'] = 'output'
df_syn.loc[df_syn['post_type'].isin(['grc']),'post_class'] = 'gr'
df_syn.loc[df_syn['post_type'].isin(['smpl']),'post_class'] = 'sp'
df_syn.loc[df_syn['post_type'].isin(['mli']),'post_class'] = 'mli'
df_syn.loc[~df_syn['post_type'].isin(['aff','mli','grc','smpl','sg1','sg2','mg1','mg2','lg','lf']),'post_class'] = 'uncategorized'


# mask = df_syn['post_type'].isin(['lf','lg','mg1','mg2','sg1','sg2'])#,'smpl','mli'])
df_pie = df_syn.groupby('post_class')['post'].count().reset_index()
df_pie['post_frac'] = (df_pie["post"] / df_pie["post"].sum()) 

# Define custom order
order = ['sg','mg','output','sp','gr','eaf','mli','uncategorized']
# Set 'post_type' as a categorical column with the desired order
df_pie['post_class'] = pd.Categorical(df_pie['post_class'], categories=order, ordered=True)

# Sort by the categorical column
df_pie = df_pie.sort_values('post_class')
df_pie


df_pie['post'].sum()


df_syn['post'].nunique()





mask = df_syn['post_class'].isin(['sg','mg','output','sp','gr','eaf','mli'])
df_pie = df_syn[mask].groupby('post_class')['post'].count().reset_index()
df_pie['post_frac'] = (df_pie["post"] / df_pie["post"].sum()) 

# Define custom order
order = ['sg','mg','output','sp','gr','eaf','mli']
# Set 'post_type' as a categorical column with the desired order
df_pie['post_class'] = pd.Categorical(df_pie['post_class'], categories=order, ordered=True)

# Sort by the categorical column
df_pie = df_pie.sort_values('post_class')
df_pie


df_pie['post'].sum()


df_syn[mask]['post'].nunique()





df_syn.loc[df_syn['post_type'].isin(['sg1','sg2']),'post_class'] = 'sg'
df_syn.loc[df_syn['post_type'].isin(['mg1','mg2']),'post_class'] = 'mg'
df_syn.loc[df_syn['post_type'].isin(['lg','lf']),'post_class'] = 'output'
df_syn.loc[df_syn['post_type'].isin(['smpl']),'post_class'] = 'sp'
df_syn.loc[df_syn['post_type'].isin(['mli']),'post_class'] = 'mli'
df_syn.loc[~df_syn['post_type'].isin(['smpl','mli','sg1','sg2','mg1','mg2','lg','lf']),'post_class'] = 'uncategorized'


# mask = df_syn['post_type'].isin(['lf','lg','mg1','mg2','sg1','sg2'])#,'smpl','mli'])
df_pie = df_syn.groupby('post_class')['post'].count().reset_index()
df_pie['post_frac'] = (df_pie["post"] / df_pie["post"].sum()) 

# Define custom order
order = ['sg','mg','output','sp','mli','uncategorized']
# Set 'post_type' as a categorical column with the desired order
df_pie['post_class'] = pd.Categorical(df_pie['post_class'], categories=order, ordered=True)

# Sort by the categorical column
df_pie = df_pie.sort_values('post_class')
df_pie


df_pie['post'].sum()


df_syn.groupby('post_class')['post'].nunique().reset_index()





mask = df_syn['post_type'].isin(['lf','lg','mg1','mg2','sg1','sg2'])#,'smpl','mli'])
df_pie = df_syn[mask].groupby('post_type')['post'].count().reset_index()
df_pie['post_frac'] = (df_pie["post"] / df_pie["post"].sum()) 

# Define custom order
order = ['lg','mg1','sg1','lf','mg2','sg2']
# Set 'post_type' as a categorical column with the desired order
df_pie['post_type'] = pd.Categorical(df_pie['post_type'], categories=order, ordered=True)

# Sort by the categorical column
df_pie = df_pie.sort_values('post_type')
df_pie


plt.figure(figsize=(1,1))
ax = plt.gca()
plt.pie(df_pie['post'], colors=[cell_colors[key] for key in df_pie['post_type']], 
                      wedgeprops=dict(edgecolor='none'),
                      startangle=0)
ax.set_aspect('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.


plt.savefig('/Users/kperks/Downloads/gca_postsyn_pie_all.svg', format='svg', transparent=True)





count_type = 'nsyn' # 'ncells' #
mask = (df_edges['post_type'].isin(['sg1','sg2','mg1','mg2','lg','lf','mli']))#,'mli','tsd','dml'])) # 

if count_type == 'nsyn':
    df_map = df_edges[mask].groupby(
        ['pre','pre_type','post_type']).sum(numeric_only=True).reset_index().pivot(
        index='pre', columns='post_type', values='weight').fillna(0).reset_index()

if count_type == 'ncells':
    df_map = df_edges[mask].groupby(
        ['pre','pre_type','post_type']).count().reset_index().pivot(
        index='pre', columns='post_type', values='post').fillna(0).reset_index()

df_map = df_map.fillna(0)
df_map = df_map.set_index('pre')


df_map.head()


target_order = ['mg1','lg','sg1','mg2','lf','sg2','mli']#['lg','mg1','sg1','lf','mg2','sg2']#
df_map = df_map.loc[:,[t for t in target_order if t in df_map.columns.values]]

# cmap = sns.color_palette("YlGnBu", as_cmap=True)  # Base colormap
# cmap.set_under('white')  # Set color for values below the colormap's minimum (i.e., 0)

# # Plot the heatmap
# fig, axes = plt.subplots(1, figsize=(5,10))
# sns.heatmap(data=df_map.sort_values(['lg']), annot=False, fmt="0.0f", cmap=cmap, 
#             ax=axes, vmin=1)  # vmin slightly above 0 to treat negative as "under"
# axes.set_title('total syn per pre cell')
# axes.set_yticks([])




mg1_map = df_map[df_map['mg1']>0].sort_values(['mg1'],ascending=False)

mg2_map = df_map[~df_map.index.isin(mg1_map.index)].sort_values(['mg2'])

sorted_full = pd.concat([mg1_map,mg2_map])

# # Plot the heatmap
# fig, axes = plt.subplots(1, figsize=(5,10))
# sns.heatmap(data=sorted_full, annot=False, fmt="0.0f", cmap=cmap, 
#             ax=axes, vmin=1)  # vmin slightly above 0 to treat negative as "under"
# axes.set_title('total syn per pre cell')
# axes.set_yticks([])


# Plotting
fig, ax = plt.subplots(figsize=(4.5, 2.25))
# Create a stacked bar plot
sorted_full.loc[:,target_order].plot.bar(stacked=True,color = cell_colors,ax=ax, width=0.9,legend=False,edgecolor='none')
ax.set_xticks([])
ax.set_yticks([0,5])
ax.set_yticklabels([])
ax.tick_params(axis='both', which='major', length=0)
sns.despine(bottom=True,trim=True,offset=2)
ax.set_ylabel('5 synapses') # this needs to match diff between ylims
ax.set_xlabel(f'GCA fibers (n={len(sorted_full)})')
# ax.legend(title='Postsynaptic \n Type', bbox_to_anchor=(1.15, 1), loc='upper right')
plt.savefig('/Users/kperks/Downloads/gca_postsyn_stacked_bar_all.svg', format='svg', transparent=True)





count_type = 'nsyn' # 'ncells' #
mask = (df_edges['pre_type'].isin(['pf']) & df_edges['post_type'].isin(['sg1','sg2','mg1','mg2','lg','lf','mli']))

df_connect = efish.get_connect_id_by_type(df_edges[mask],count_type)

df_connect.head()





target_order = ['lg','mg1','sg1','lf','mg2','sg2','mli']#,'smpl','mli','h','tsd']
df_connect = df_connect.loc[:,[t for t in target_order if t in df_connect.columns.values]]


df_connect


df_connect.sum()


cmap = sns.color_palette("YlGnBu", as_cmap=True)  # Base colormap
cmap.set_under('white')  # Set color for values below the colormap's minimum (i.e., 0)

# Plot the heatmap
fig, axes = plt.subplots(1, figsize=(3,5))
sns.heatmap(data=df_connect.sort_values(['lg']), annot=False, fmt="0.0f", cmap=cmap, 
            ax=axes, vmin=1)  # vmin slightly above 0 to treat negative as "under"
axes.set_title('total syn per pre cell')
axes.set_yticks([])


sort1 = 'mg1'
sort1_connect = df_connect[df_connect[sort1]>0].sort_values([sort1],ascending=False)

sort2 = 'mg2'
sort2_connect = df_connect[~df_connect.index.isin(sort1_connect.index)].sort_values([sort2])

other_ = df_connect[(~df_connect.index.isin(sort1_connect.index)) & ~df_connect.index.isin(sort2_connect.index)]#.sort_values(['mg2'],ascending=False)

sorted_full = pd.concat([sort1_connect,other_,sort2_connect])


# # Plot the heatmap
# fig, axes = plt.subplots(1, figsize=(3,5))
# sns.heatmap(data=sorted_full, annot=False, fmt="0.0f", cmap=cmap, 
#             ax=axes, vmin=1)  # vmin slightly above 0 to treat negative as "under"
# axes.set_title('total syn per pre cell')
# axes.set_yticks([])


# Plotting
fig, ax = plt.subplots(figsize=(8, 3))
# Create a stacked bar plot
sorted_full.loc[:,target_order].plot.bar(stacked=True,color = syn_colors,ax=ax, width=0.9)
ax.set_xticks([])
ax.legend(title='Postsynaptic \n Type', bbox_to_anchor=(1.35, 1), loc='upper right')
plt.savefig('/Users/kperks/Downloads/pf_output_stacked_bar_with-mli.svg', format='svg', transparent=True)





def get_conditional_output(df_syn,order, normalize=False):
    '''get p(connect)'''
    df_edges=df_syn[['pre','post','pre_type','post_type']].value_counts().reset_index(name='weight')
    df_map = df_edges.groupby(['pre','pre_type','post_type']).sum(numeric_only=True).reset_index().pivot(index='pre', columns='post_type', values='weight').fillna(0).reset_index().set_index('pre')
    df_map = df_map[order]

    result = []
    for g in df_map.columns:
        # g = 'aff'
        df_sub = deepcopy(df_map[(df_map[g] > 0)])
        df_sub.loc[:,g] = df_sub[g].values-1 # subtract the one connection that qualifies this cell as getting input from g type

        if normalize==True:
            df_sub = df_sub.div(df_sub.sum(axis=1),axis=0)

        result.append(list(df_sub.mean().values)) 

    order = df_map.columns
        
    return result,order



pre_groups = ['pf']
post_groups = ['sg1','mg1','lg','sg2','mg2','lf']


df_counts = df_syn.groupby(['pre'])['post'].count().reset_index()
pre_highsyn = df_counts[df_counts['post']>4]['pre'].values
pre_lowsyn = df_counts[df_counts['post']<20]['pre'].values


mask = df_syn['pre_type'].isin(pre_groups) & df_syn['post_type'].isin(post_groups) #& \
    #df_syn['pre'].isin(pre_highsyn) #& df_syn['pre'].isin(pre_lowsyn)  # Filter out rows with post_type not in post_types_order and pre_tyep not in 





result_shuff = []

# Iterate the specified number of times
for i in range(100):
    
    df_syn_shuff = efish.shuffle_synapses(df_syn[mask],['post','post_type']) #post ID must travel in syn table with its ID for the rest of the analysis to work

    result_,order = get_conditional_output(df_syn_shuff,order = post_groups, normalize=True)
    result_ = np.asarray(result_)
    
    # Append the result as a row to the result_df
    result_shuff.append(result_)

result_shuff = np.asarray(result_shuff)
u_mat = result_shuff.mean(axis=0)
std_mat = result_shuff.std(axis=0)





result_data,order = get_conditional_output(df_syn[mask], order = post_groups,normalize=True)

# Calculate the z-scores
z_scores = (result_data - u_mat) / std_mat
z_scores[np.isclose(std_mat, 0)] = 0  # Replace z-scores with 0 where std is 0 # Handle cases where std_2d is zero to avoid division by zero

cond_input_mat = pd.DataFrame(z_scores,columns = order, index = order)


# Ensure the color range is centered around 0
vmin = -max(abs(cond_input_mat.min().min()), abs(cond_input_mat.max().max()))
vmax = -vmin

sns.set_context("paper",font_scale=1)
hfig,ax = plt.subplots(1,figsize=(2,2))
sns.heatmap(cond_input_mat,
    cmap="RdBu_r",  # Diverging colormap from red to blue
    vmin=vmin,
    vmax=vmax,
    center=0)

plt.savefig('/Users/kperks/Downloads/pf_output_conditional_output_analysis_all.svg', format='svg', transparent=True)









df_syn_shuff = efish.shuffle_synapses(df_syn[mask],['post','post_type']) #post ID must travel in syn table with its ID for the rest of the analysis to work

result_,order = get_conditional_output(df_syn_shuff,order = post_groups, normalize=True)
result_ = np.asarray(result_)

cond_input_mat = pd.DataFrame(result_,columns = order, index = order)

# Ensure the color range is centered around 0
vmin = -max(abs(cond_input_mat.min().min()), abs(cond_input_mat.max().max()))
vmax = -vmin

hfig,ax = plt.subplots(1,figsize=(2,2))
sns.heatmap(cond_input_mat,
    cmap="RdBu_r",  # Diverging colormap from red to blue
    vmin=vmin,
    vmax=vmax,
    center=0, ax=ax)





from pca import pca

def calculate_percent_total(row):
    total = row.sum()
    if total == 0:
        return row  # If total is 0, return the row as is (unchanged)
    else:
        return row / total  # Normalize by dividing each element by the total
#row / total 


df_syn = pd.read_csv(dirpath / 'graphs/df_pfsyn.csv')
syn = 'post-synaptic'


for i,r in df_syn.iterrows():
    try:
        df_syn.loc[i,'pre_type'] =df_type[df_type['id'].isin([r['pre']])].cell_type.values[0]
        df_syn.loc[i,'post_type']=df_type[df_type['id'].isin([r['post']])].cell_type.values[0]
    except:
        print(r['pre'],r['post'])
        continue

df_syn.loc[:,'post_type'] = [t.lower() for t in df_syn['post_type']]
df_syn.loc[:,'pre_type'] = [t.lower() for t in df_syn['pre_type']]


# include only connections to the following types/groups of cells
included_groups = ['sg1','sg2','mg1','mg2','lg','lf']
df_syn = df_syn[(df_syn['post_type'].isin(included_groups))] #]#
len(df_syn)





df_edges=df_syn.drop(['Unnamed: 0','x','y','z','structure'],axis=1).value_counts().reset_index(name='weight')

# Filter out rows with post_type not in post_types_order
df_edges_filtered = df_edges[df_edges[dsource].isin(source_types_order)]

# Pivot table to get sum of weights per post and pre_type
pivot_df = df_edges_filtered.pivot_table(values='weight', index=[idsource, dsource], columns=dtarget, aggfunc='sum', fill_value=0)

# Reindex columns to include only the specified pre_types in the desired order
pivot_df = pivot_df.reindex(columns=target_types_order, fill_value=0)

# Reset index to turn 'post' and 'post_type' from index to columns
pivot_df.reset_index(inplace=True)

# Sort the dataframe based on post_type
pivot_df[dsource] = pd.Categorical(pivot_df[dsource], categories=source_types_order, ordered=True)
pivot_df.sort_values(dsource, inplace=True)
pivot_df.set_index(idsource, inplace=True)

# Apply the function to each row
# norm_df = pivot_df.drop(columns=dsource).apply(calculate_percent_total, axis=1)
norm_df = pivot_df.select_dtypes(include='number').apply(calculate_percent_total, axis=1)

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(norm_df)


scaled_data = pd.DataFrame(scaled_data,columns=norm_df.columns)


scaled_data.head()


model = pca()


results = model.fit_transform(scaled_data)


model.plot(figsize=(5,5))


X_feat = norm_df['mg2'].values>np.mean(norm_df['mg2'].values)
model.biplot(figsize=(10, 10), dpi=50,labels=X_feat)
plt.savefig('/Users/kperks/Downloads/pf_output_PCA_loadings_labeled-by-mg2.svg', format='svg', transparent=True)


X_feat = norm_df['mg1'].values>np.mean(norm_df['mg1'].values)
model.biplot(figsize=(10, 10), dpi=50,labels=X_feat)
plt.savefig('/Users/kperks/Downloads/pf_output_PCA_loadings_labeled-by-mg1.svg', format='svg', transparent=True)


X_feat = norm_df['lg'].values>np.mean(norm_df['lg'].values)
model.biplot(figsize=(10, 10), dpi=50,labels=X_feat)



# Grap the values for mg2 syn and # Color based on mean
X_feat = norm_df['mg2'].values>np.mean(norm_df['mg2'].values)

# Scatter based on discrete color
model.scatter3d(labels=X_feat, title='Color on flavanoids (Gray colored samples are > mean)',figsize=(10,10),dpi=50)

# 3d scatter plot
# model.scatter3d(labels=color_label, title='Color on flavanoids (Gray colored samples are > mean)')








df_syn_rand = deepcopy(df_syn)





# post-synaptic labeling
dsource = 'pre_type'
idsource = 'pre'
dtarget = 'post_type'

# Desired pre_types and order
source_types_order = ['pf']#
# Desired post_types order
target_types_order =  ['sg1', 'mg1','lg', 'sg2','mg2','lf'] #['aff',  'sg1','mg1', 'grc', 'sg2','mg2']





df_edges=df_syn.drop(['Unnamed: 0','x','y','z','structure'],axis=1).value_counts().reset_index(name='weight')

# Filter out rows with post_type not in post_types_order
df_edges_filtered = df_edges[df_edges[dsource].isin(source_types_order)]

# Pivot table to get sum of weights per post and pre_type
pivot_df = df_edges_filtered.pivot_table(values='weight', index=[idsource, dsource], columns=dtarget, aggfunc='sum', fill_value=0)

# Reindex columns to include only the specified pre_types in the desired order
pivot_df = pivot_df.reindex(columns=target_types_order, fill_value=0)

# Reset index to turn 'post' and 'post_type' from index to columns
pivot_df.reset_index(inplace=True)

# Sort the dataframe based on post_type
pivot_df[dsource] = pd.Categorical(pivot_df[dsource], categories=source_types_order, ordered=True)
pivot_df.sort_values(dsource, inplace=True)
pivot_df.set_index(idsource, inplace=True)

# Apply the function to each row
# norm_df = pivot_df.drop(columns=dsource).apply(calculate_percent_total, axis=1)
norm_df = pivot_df.select_dtypes(include='number').apply(calculate_percent_total, axis=1)

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(norm_df)

# Perform PCA
npc = 6
pca = PCA(n_components=npc)  # You can change the number of components as needed

pca_result = pca.fit_transform(scaled_data)

# get loadings of dimensions onto each principal component
loadings = pca.components_.T

# Add PCA results to the DataFrame
for i in range(npc):
    pivot_df[str(i)] = pca_result[:, i]


result_data = pd.DataFrame(pca.explained_variance_ratio_.reshape(1, -1),columns=range(len(pca.explained_variance_ratio_)))


pivot_df


hfig,ax = plt.subplots(1,figsize=(2,2))
sns.scatterplot(ax=ax,data=pivot_df,x='0',y='1',linewidth=1,color='gray',edgecolor='black',s=50,alpha = 0.75,hue='mg1')

# plt.show()
plt.savefig(f'/Users/kperks/Downloads/pf_pca.svg', format='svg', transparent=True)
sns.move_legend(ax, "upper left", bbox_to_anchor=(1, 1))


hfig,ax = plt.subplots(figsize = (2,2))
ax.scatter(['sg1','mg1','lg','sg2','mg2','lf'],[l[0] for l in loadings],color = 'black',s=50)





# total_syn = pivot_df[['sg1','mg1','lg','sg2','mg2','lf']].sum(axis=1)
# pivot_df.loc[:,'total_syn'] = total_syn
pivot_df.loc[:,'type1'] = pivot_df[['sg1','mg1','lg']].sum(axis=1)#/total_syn
pivot_df.loc[:,'type2'] = pivot_df[['sg2','mg2','lf']].sum(axis=1)#/total_syn


pivot_df.head()


hfig,ax = plt.subplots(figsize=(3,3))
sns.scatterplot(ax=ax,data=pivot_df,x='type1',y='type2',color = 'black',alpha = 0.5)





result_df = pd.DataFrame()

# Iterate the specified number of times
for i in range(50):
    # Shuffle the dataframe
    # df_syn_rand.loc[:,['pre']] = df_syn_rand['pre'].sample(frac = 1).values ## *** this does not work unless you re-type the pre_type column after***
    shuff_rows = df_syn_rand[['pre','pre_type']].sample(frac = 1)
    df_syn_rand.loc[:,['pre']] = shuff_rows['pre'].values
    df_syn_rand.loc[:,['pre_type']] = shuff_rows['pre_type'].values
    
    df_edges=df_syn_rand.drop(['Unnamed: 0','x','y','z','structure'],axis=1).value_counts().reset_index(name='weight')
    # Filter out rows with post_type not in post_types_order
    df_edges_filtered = df_edges[df_edges[dsource].isin(source_types_order)]
    
    # Pivot table to get sum of weights per post and pre_type
    pivot_df = df_edges_filtered.pivot_table(values='weight', index=[idsource, dsource], columns=dtarget, aggfunc='sum', fill_value=0)
    
    # Reindex columns to include only the specified pre_types in the desired order
    pivot_df = pivot_df.reindex(columns=target_types_order, fill_value=0)
    
    # Reset index to turn 'post' and 'post_type' from index to columns
    pivot_df.reset_index(inplace=True)
    
    # Sort the dataframe based on post_type
    pivot_df[dsource] = pd.Categorical(pivot_df[dsource], categories=source_types_order, ordered=True)
    pivot_df.sort_values(dsource, inplace=True)
    pivot_df.set_index(idsource, inplace=True)
    
    # Apply the function to each row
    # norm_df = pivot_df.drop(columns=dsource).apply(calculate_percent_total, axis=1)
    norm_df = pivot_df.select_dtypes(include='number').apply(calculate_percent_total, axis=1)
    
    # Standardize the data
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(norm_df)
    
    # Perform PCA
    npc = 6
    pca = PCA(n_components=npc)  # You can change the number of components as needed
    pca_result = pca.fit_transform(scaled_data)

        # get loadings of dimensions onto each principal component
    loadings = pca.components_.T
    
    # Add PCA results to the DataFrame
    for i in range(npc):
        pivot_df[str(i)] = pca_result[:, i]
    
    # Append the result as a row to the result_df
    result_df = pd.concat([result_df, pd.DataFrame(pca.explained_variance_ratio_.reshape(1, -1))], ignore_index=True)


hfig,ax = plt.subplots(1,figsize=(2,2))
sns.scatterplot(ax=ax,data=pivot_df,x='0',y='1',linewidth=1,color='gray',edgecolor='black',s=50,alpha = 0.75)
# sns.move_legend(ax, "upper left", bbox_to_anchor=(1, 1))
# plt.show()
plt.savefig(f'/Users/kperks/Downloads/pf_pca_shuffle.svg', format='svg', transparent=True)


hfig,ax = plt.subplots(figsize = (2,2))
ax.scatter(['sg1','mg1','lg','sg2','mg2','lf'],[l[0] for l in loadings],color = 'black',s=50)





hfig,ax = plt.subplots(1,figsize=(1.5,3))

# plot single iteration, individual cells
# sns.pointplot(data = df_norm,linestyle='',color = 'black',markersize = 5,order = ['lg','mg1','sg1','lf','mg2','sg2'])
# sns.pointplot(data=df_norm_data[(df_norm_data[by_group] != 0)],linestyle='',color = 'red',markersize = 5,order = ['lg','mg1','sg1','lf','mg2','sg2'])

# plot iteration means
sns.pointplot(data = result_df,linestyle='',color = 'black',markersize = 5,errorbar='ci')
sns.pointplot(data=result_data,linestyle='',color = 'red',markersize = 5)

ax.set_ylabel('explained variance')
plt.savefig(f'/Users/kperks/Downloads/pf_output_pca_compare.svg', format='svg', transparent=True)








df_syn = pd.read_csv(dirpath / 'graphs/df_postsyn_mli.csv')
syn = 'post-synaptic'


len(df_syn)


y_adj_col = []
for i,r in df_syn.iterrows():
    yoffset = func((r['x'], r['z']), *popt)
    y_adj = (r['y'] - yoffset)
    y_adj_col.append(y_adj)

df_syn.loc[:,'y_adj']=y_adj_col

for v in ['x','y','z','y_adj']:
    df_syn[v] = df_syn[v]/1000
df_syn['y_adj'] = df_syn['y_adj']*-1    


# pf_df = deepcopy(df_syn[(df_syn['pre_type'].isin(['pf']))&(df_syn['post_type'].isin(['mg1','mg2','lg','lf']))])
# pf_df['x']=pf_df['x'].apply(lambda x: x/16)
# pf_df['y']=pf_df['y'].apply(lambda x: x/16)
# pf_df['z']=pf_df['z'].apply(lambda x: x/30)

# pf_df.to_csv(Path('/Users/kperks/Downloads/pf_synapses.csv'))





for i,r in df_syn.iterrows():
    try:
        df_syn.loc[i,'pre_type'] =df_type[df_type['id'].isin([r['pre']])].cell_type.values[0]
        df_syn.loc[i,'post_type']=df_type[df_type['id'].isin([r['post']])].cell_type.values[0]
    except:
        print(r['pre'],r['post'])
        continue

df_syn.loc[:,'post_type'] = [t.lower() for t in df_syn['post_type']]
df_syn.loc[:,'pre_type'] = [t.lower() for t in df_syn['pre_type']]


# include only connections to the following types/groups of cells
included_groups = ['mli']#['sg1','sg2','mg1','mg2','lg','lf','tsd','h','mli']#,'dml']
df_syn = df_syn[(df_syn['pre_type'].isin(included_groups))]#& df_syn['pre_type'].isin(['mg1','mg2'])]
len(df_syn)


df_syn['pre'].unique()





df_syn.head()


df_edges=df_syn[['pre','post','pre_type','post_type']].value_counts().reset_index(name='weight')





syn = 'post-synaptic'


df_edges['post_type'].unique()


all_types = set(df_edges['pre_type'].unique()) | set(df_edges['post_type'].unique()) #- set(['glia'])
c_type_check = all_types #['mg1','mg2','sg1','sg2','grc']

df_progress = pd.DataFrame(columns = ['id','cell_type','n_syn','done','todo','completed']) #'soma-diam',
for c in df_edges['pre'].unique():
    
    if (df_edges[df_edges['pre'] == c]['pre_type'].unique()[0] in c_type_check):

        c_df = df_edges[df_edges['pre'].isin([c])]
        n_syn_done = c_df['weight'].sum()

        cell = ecrest(settings_dict,filepath = nodefiles[str(c)],launch_viewer=False)
        
        if len(cell.cell_data['end_points'][syn])>0:
            # try:
            cell_dict = {
                'id': cell.cell_data['metadata']['main_seg']['base'],
                'cell_type': df_type[df_type['id']==c]['cell_type'].values[0],#cell_type[c], #cell.get_ctype("manual"),
                # 'soma-diam': soma_diam[c],
                'n_syn': len(cell.cell_data['end_points'][syn]),
                'done': n_syn_done, #len(c_df),
                'todo': len(cell.cell_data['end_points'][syn]) - n_syn_done, #len(c_df),
                'completed': n_syn_done / (len(cell.cell_data['end_points'][syn]))
                }
            # except:
            #     print(f'error on {c}')
            #     continue
        
        if len(cell.cell_data['end_points'][syn])==0:
            cell_dict = {
                'id': cell.cell_data['metadata']['main_seg']['base'],
                'cell_type': df_type[df_type['id']==c]['cell_type'].values[0],#cell_type[c], #cell.get_ctype("manual"),
                # 'soma-diam': soma_diam[c],
                'n_syn': np.NaN,
                'done': n_syn_done, #len(c_df),
                'todo': np.NaN, #len(c_df),
                'completed': np.NaN
                }
            # print(f'error on {c}')
            # continue

        cell_df = pd.DataFrame([cell_dict]).dropna(how='all')
        if not cell_df.empty:
            df_progress = pd.concat([df_progress, cell_df], ignore_index=True)

        # df_progress = pd.concat([df_progress,pd.DataFrame([cell_dict])])


pd.set_option('display.max_rows', None)


df_progress[(df_progress['cell_type'].isin(['mli']))].sort_values('n_syn')# & (df_progress['todo']>0) #grc','sgx2','sg2','mg2





hfig,ax = plt.subplots(1,figsize=(4,3))
sns.histplot(data=df_edges[df_edges['post_type'].isin(['lg','lf','mg1','mg2','sg1','sg2'])],x='weight',color='black',ax=ax,bins=np.arange(0.5,16.5))
ax.set_xticks(np.arange(0,17,2));


hfig,ax = plt.subplots(1,figsize=(4,3))
sns.histplot(data=df_edges[df_edges['post_type'].isin(['mli'])],x='weight',color='black',ax=ax,bins=np.arange(0.5,16.5))
ax.set_xticks(np.arange(0,17,2));





df_syn['depth'] = pd.cut(df_syn['y_adj'], bins=np.arange(0,350,50))


df_syn.head()


mask = df_syn['post_type'].isin(['sg1','sg2','mg1','mg2','lg','lf','smpl','grc','mli','tsd','h'])# df_edges['pre'].isin([295969348,295969442,295969134,295969355,295968777,282228761,283375247,283391297,283390956,282230475,268614458,268614383,273086215,187230424,290552453,27220895,31694533,102463116,188296613,15401313,17877032,187151336,117041378,122039969,36165549]) &  


df_grouped = df_syn[mask][['pre','post_type','depth']].groupby(['depth','post_type'],observed=False).count().reset_index()


df_grouped.head()#[df_grouped['weight']>0]


df_grouped.loc[df_grouped['post_type'].isin(['sg1','sg2']),'class'] = 'sg'
df_grouped.loc[df_grouped['post_type'].isin(['mg1','mg2']),'class'] = 'mg'
df_grouped.loc[df_grouped['post_type'].isin(['lf','lg']),'class'] = 'output'
df_grouped.loc[df_grouped['post_type'].isin(['smpl','mli','tsd','h']),'class'] = 'smooth'


df_grouped['weight_norm'] = df_grouped.groupby('depth',observed=False)['pre'].transform(lambda x: (x / x.sum()) )

df_grouped['weight_all'] = df_grouped.groupby('depth',observed=False)['pre'].transform(lambda x: (x.sum()) )



# Create a seaborn plot
hfig,ax = plt.subplots(1,figsize=(2,5))
sns.pointplot(data=df_grouped,y='depth', x='weight_norm',hue = 'class',linewidth=1)#,order = ['lg','mg1','sg1','lf','mg2','sg2'])
ax.invert_yaxis()
plt.savefig('/Users/kperks/Downloads/PFsynapse_locations_binned_normalized.svg',transparent=True) 


# sns.pointplot(data=df_grouped,x='depth', y='weight_all',color='black')
hfig,ax = plt.subplots(1,figsize=(2,5))
sns.pointplot(data=df_grouped,y='depth', x='weight_all',color='black',linewidth=1)#,order = ['lg','mg1','sg1','lf','mg2','sg2'])
ax.invert_yaxis()
plt.savefig('/Users/kperks/Downloads/PFsynapse_locations_binned_TOTAL.svg',transparent=True) 






g = sns.jointplot(data=df_syn[df_syn['post_type'].isin(['mg2','mg1','sg2','sg1','lg','lf','smpl','mli','tsd','h'])], #['aff','sg2','mg2','sg1','mg1','grc-d','grc-s']
                  x="x", y="y_adj", hue="post_type", height=10,
                  alpha = 0.5, edgecolor='black',palette = syn_colors,
                 marginal_kws=dict(common_norm=True,clip_on=0, fill=False), space=0)
# sns.scatterplot(x=[0],y=[0],color='black',s=100);

sns_joint_equal_axes(g);
sns.move_legend(plt.gca(), "upper right", bbox_to_anchor=(1.4, 1));

# g = sns.jointplot(data=df_syn[df_syn['pre_type'].isin(['mg2','mg1'])], #['aff','sg2','mg2','sg1','mg1','grc-d','grc-s']
#                   x="x", y="z", hue="pre_type", height=10,
#                   alpha = 0.5, edgecolor='black',palette = syn_colors,
#                  marginal_kws=dict(common_norm=True,clip_on=0, fill=False), space=0)
# # sns.scatterplot(x=[0],y=[0],color='black',s=100);

# sns_joint_equal_axes(g);
# sns.move_legend(plt.gca(), "upper right", bbox_to_anchor=(1.4, 1));





count_type = 'nsyn' # 'ncells' #
mask = (df_edges['post_type'].isin(['sg1','sg2','mg1','mg2','lg','lf','mli','smpl']))#,'smpl','mli','tsd','h'])) # df_edges['pre'].isin([290552453,27220895,31694533,102463116,188296613,15401313,17877032,187151336,117041378,122039969,36165549]) & 
#[295969348,295969442,295969134,295969355,295968777,282228761,283375247, 283391297,283390956,282230475,268614458,268614383,273086215,187230424]

types_ = ['mli'] #should be all that is in df_pfsyn anyway
df_map = pd.DataFrame()
for t in types_:
    if count_type == 'nsyn':
        df_grouped = df_edges[(df_edges['pre_type']==t) & mask].groupby(
            ['pre','pre_type','post_type']).sum(numeric_only=True).reset_index().pivot(
            index='pre', columns='post_type', values='weight').fillna(0).reset_index()
    
    if count_type == 'ncells':
        df_grouped = df_edges[(df_edges['pre_type']==t) & mask].groupby(
            ['pre','pre_type','post_type']).count().reset_index().pivot(
            index='pre', columns='post_type', values='post').fillna(0).reset_index()
    
    df_grouped['pre_type']=t
    df_map = pd.concat([df_map,df_grouped])
    
df_map = df_map.fillna(0)
df_map = df_map.set_index('pre')
df_map = df_map.drop(['pre_type'],axis=1)


df_map#.head()








target_order = ['lg','mg1','sg1','lf','mg2','sg2','mli','smpl']#,'smpl','mli','h','tsd']
df_map = df_map.loc[:,[t for t in target_order if t in df_map.columns.values]]


df_map.sum()


cmap = sns.color_palette("YlGnBu", as_cmap=True)  # Base colormap
cmap.set_under('white')  # Set color for values below the colormap's minimum (i.e., 0)

# Ensure all 0s are treated as "under" the colormap range
# df_map = df_map.replace(0, -1)  # Replace 0s with a value below the colormap range

# fig, axes = plt.subplots(1, figsize=(5,10))
# sns.heatmap(data=df_map.sort_values(['lg']), annot=False, fmt="0.0f", cmap="YlGnBu", ax=axes)#  .sort_index()
# # sns.heatmap(data=df_map.div(df_map.sum(axis=1),axis=0).mul(100).round(0), annot=True, fmt="0.0f", cmap="YlGnBu", ax=axes[1])
# axes.set_title('total syn per pre cell')

# Plot the heatmap
fig, axes = plt.subplots(1, figsize=(5,10))
sns.heatmap(data=df_map.sort_values(['lg']), annot=False, fmt="0.0f", cmap=cmap, 
            ax=axes, vmin=1)  # vmin slightly above 0 to treat negative as "under"
axes.set_title('total syn per pre cell')
axes.set_yticks([])


sort1 = 'mg1'
sort1_map = df_map[df_map[sort1]>0].sort_values([sort1],ascending=False)


sort2 = 'mg2'
sort2_map = df_map[~df_map.index.isin(sort1_map.index)].sort_values([sort2])


other_map = df_map[(~df_map.index.isin(sort1_map.index)) & ~df_map.index.isin(sort2_map.index)]#.sort_values(['mg2'],ascending=False)


sorted_full = pd.concat([sort1_map,other_map,sort2_map])


# Plot the heatmap
fig, axes = plt.subplots(1, figsize=(5,10))
sns.heatmap(data=sorted_full, annot=False, fmt="0.0f", cmap=cmap, 
            ax=axes, vmin=1)  # vmin slightly above 0 to treat negative as "under"
axes.set_title('total syn per pre cell')
axes.set_yticks([])


# Plotting
fig, ax = plt.subplots(figsize=(5, 5))
# Create a stacked bar plot
sorted_full.loc[:,target_order].plot.bar(stacked=True,color = syn_colors,ax=ax, width=0.9)
# ax.set_xticks([])
ax.legend(title='Postsynaptic \n Type', bbox_to_anchor=(1.35, 1), loc='upper right')
plt.savefig('/Users/kperks/Downloads/mli_output_stacked_bar.svg', format='svg', transparent=True)





def calculate_percent_total(row):
    total = row.sum()
    if total == 0:
        return row  # If total is 0, return the row as is (unchanged)
    else:
        return row / total  # Normalize by dividing each element by the total
#row / total 


df_syn = pd.read_csv(dirpath / 'graphs/df_postsyn_mli.csv')
syn = 'post-synaptic'


for i,r in df_syn.iterrows():
    try:
        df_syn.loc[i,'pre_type'] =df_type[df_type['id'].isin([r['pre']])].cell_type.values[0]
        df_syn.loc[i,'post_type']=df_type[df_type['id'].isin([r['post']])].cell_type.values[0]
    except:
        print(r['pre'],r['post'])
        continue

df_syn.loc[:,'post_type'] = [t.lower() for t in df_syn['post_type']]
df_syn.loc[:,'pre_type'] = [t.lower() for t in df_syn['pre_type']]


# include only connections to the following types/groups of cells
included_groups = ['sg1','sg2','mg1','mg2','lg','lf']
df_syn = df_syn[(df_syn['post_type'].isin(included_groups))] #]#
len(df_syn)





df_syn_rand = deepcopy(df_syn)





# post-synaptic labeling
dsource = 'pre_type'
idsource = 'pre'
dtarget = 'post_type'

# Desired pre_types and order
source_types_order = ['mli']#
# Desired post_types order
target_types_order =  ['sg1', 'mg1','lg', 'sg2','mg2','lf','mli'] #['aff',  'sg1','mg1', 'grc', 'sg2','mg2']





df_edges=df_syn.drop(['Unnamed: 0','x','y','z','structure'],axis=1).value_counts().reset_index(name='weight')

# Filter out rows with post_type not in post_types_order
df_edges_filtered = df_edges[df_edges[dsource].isin(source_types_order)]

# Pivot table to get sum of weights per post and pre_type
pivot_df = df_edges_filtered.pivot_table(values='weight', index=[idsource, dsource], columns=dtarget, aggfunc='sum', fill_value=0)

# Reindex columns to include only the specified pre_types in the desired order
pivot_df = pivot_df.reindex(columns=target_types_order, fill_value=0)

# Reset index to turn 'post' and 'post_type' from index to columns
pivot_df.reset_index(inplace=True)

# Sort the dataframe based on post_type
pivot_df[dsource] = pd.Categorical(pivot_df[dsource], categories=source_types_order, ordered=True)
pivot_df.sort_values(dsource, inplace=True)
pivot_df.set_index(idsource, inplace=True)

# Apply the function to each row
# norm_df = pivot_df.drop(columns=dsource).apply(calculate_percent_total, axis=1)
norm_df = pivot_df.select_dtypes(include='number').apply(calculate_percent_total, axis=1)

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(norm_df)

# Perform PCA
npc = 7
pca = PCA(n_components=npc)  # You can change the number of components as needed

pca_result = pca.fit_transform(scaled_data)

# get loadings of dimensions onto each principal component
loadings = pca.components_.T

# Add PCA results to the DataFrame
for i in range(npc):
    pivot_df[str(i)] = pca_result[:, i]


hfig,ax = plt.subplots(1,figsize=(2,2))
sns.scatterplot(ax=ax,data=pivot_df,x='0',y='1',linewidth=1,color='gray',edgecolor='black',s=50,alpha = 0.75)
# sns.move_legend(ax, "upper left", bbox_to_anchor=(1, 1))
# plt.show()
plt.savefig(f'/Users/kperks/Downloads/mli_pca.svg', format='svg', transparent=True)


hfig,ax = plt.subplots(figsize = (2,2))
ax.scatter(['sg1','mg1','lg','sg2','mg2','lf','mli'],[l[0] for l in loadings],color = 'black',s=50)


result_data = pd.DataFrame(pca.explained_variance_ratio_.reshape(1, -1),columns=range(len(pca.explained_variance_ratio_)))





result_df = pd.DataFrame()

# Iterate the specified number of times
for i in range(50):
    # Shuffle the dataframe
    # df_syn_rand.loc[:,['pre']] = df_syn_rand['pre'].sample(frac = 1).values ## *** this does not work unless you re-type the pre_type column after***
    shuff_rows = df_syn_rand[['pre','pre_type']].sample(frac = 1)
    df_syn_rand.loc[:,['pre']] = shuff_rows['pre'].values
    df_syn_rand.loc[:,['pre_type']] = shuff_rows['pre_type'].values
    
    df_edges=df_syn_rand.drop(['Unnamed: 0','x','y','z','structure'],axis=1).value_counts().reset_index(name='weight')
    # Filter out rows with post_type not in post_types_order
    df_edges_filtered = df_edges[df_edges[dsource].isin(source_types_order)]
    
    # Pivot table to get sum of weights per post and pre_type
    pivot_df = df_edges_filtered.pivot_table(values='weight', index=[idsource, dsource], columns=dtarget, aggfunc='sum', fill_value=0)
    
    # Reindex columns to include only the specified pre_types in the desired order
    pivot_df = pivot_df.reindex(columns=target_types_order, fill_value=0)
    
    # Reset index to turn 'post' and 'post_type' from index to columns
    pivot_df.reset_index(inplace=True)
    
    # Sort the dataframe based on post_type
    pivot_df[dsource] = pd.Categorical(pivot_df[dsource], categories=source_types_order, ordered=True)
    pivot_df.sort_values(dsource, inplace=True)
    pivot_df.set_index(idsource, inplace=True)
    
    # Apply the function to each row
    # norm_df = pivot_df.drop(columns=dsource).apply(calculate_percent_total, axis=1)
    norm_df = pivot_df.select_dtypes(include='number').apply(calculate_percent_total, axis=1)
    
    # Standardize the data
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(norm_df)
    
    # Perform PCA
    npc = 7
    pca = PCA(n_components=npc)  # You can change the number of components as needed
    pca_result = pca.fit_transform(scaled_data)

        # get loadings of dimensions onto each principal component
    loadings = pca.components_.T
    
    # Add PCA results to the DataFrame
    for i in range(npc):
        pivot_df[str(i)] = pca_result[:, i]
    
    # Append the result as a row to the result_df
    result_df = pd.concat([result_df, pd.DataFrame(pca.explained_variance_ratio_.reshape(1, -1))], ignore_index=True)


hfig,ax = plt.subplots(1,figsize=(2,2))
sns.scatterplot(ax=ax,data=pivot_df,x='0',y='1',linewidth=1,color='gray',edgecolor='black',s=50,alpha = 0.75)
# sns.move_legend(ax, "upper left", bbox_to_anchor=(1, 1))
# plt.show()
plt.savefig(f'/Users/kperks/Downloads/mli_pca_shuffle.svg', format='svg', transparent=True)


hfig,ax = plt.subplots(figsize = (2,2))
ax.scatter(['sg1','mg1','lg','sg2','mg2','lf','mli'],[l[0] for l in loadings],color = 'black',s=50)





hfig,ax = plt.subplots(1,figsize=(1.5,3))

# plot single iteration, individual cells
# sns.pointplot(data = df_norm,linestyle='',color = 'black',markersize = 5,order = ['lg','mg1','sg1','lf','mg2','sg2'])
# sns.pointplot(data=df_norm_data[(df_norm_data[by_group] != 0)],linestyle='',color = 'red',markersize = 5,order = ['lg','mg1','sg1','lf','mg2','sg2'])

# plot iteration means
sns.pointplot(data = result_df,linestyle='',color = 'black',markersize = 5,errorbar='ci')
sns.pointplot(data=result_data,linestyle='',color = 'red',markersize = 5)

ax.set_ylabel('explained variance')
plt.savefig(f'/Users/kperks/Downloads/mli_output_pca_compare.svg', format='svg', transparent=True)





def get_conditional_output(df_edges,normalize=False):
    '''get p(connect)'''
    df_map = df_edges.groupby(['pre','pre_type','post_type']).sum(numeric_only=True).reset_index().pivot(index='pre', columns='post_type', values='weight').fillna(0).reset_index().set_index('pre')

    if normalize==True:
        df_map = df_map.div(df_map.sum(axis=1),axis=0)
    
        '''group data'''
        result = []
        for g in df_map.columns:
            result.append(list(df_map[(df_map[g] > 0.05)].mean().values))

    if normalize==False:
        '''group data'''
        result = []
        for g in df_map.columns:
            result.append(list(df_map[(df_map[g] > 1)].mean().values))

    order = df_map.columns
        
    return result,order


pre_groups = ['mli']
post_groups = ['mg1','mg2','lg','lf','sg1','sg2','mli']


df_syn





result_shuff = []

# Iterate the specified number of times
for i in range(100):
    df_syn_shuff = deepcopy(df_syn)
    mask = df_syn_shuff['pre_type'].isin(pre_groups) & df_syn_shuff['post_type'].isin(post_groups)  # Filter out rows with post_type not in post_types_order and pre_tyep not in pre_types_order
    df_syn_shuff = df_syn_shuff[mask]
    # Shuffle the dataframe
    # df_syn_rand.loc[:,['pre']] = df_syn_rand['pre'].sample(frac = 1).values ## *** this does not work unless you re-type the pre_type column after***
    shuff_rows = df_syn_shuff[['post','x','y','z','y_adj','post_type']].sample(frac = 1)
    df_syn_shuff.loc[:,['post']] = shuff_rows['post'].values
    df_syn_shuff.loc[:,['x']] = shuff_rows['x'].values
    df_syn_shuff.loc[:,['y']] = shuff_rows['y'].values
    df_syn_shuff.loc[:,['z']] = shuff_rows['z'].values
    df_syn_shuff.loc[:,['y_adj']] = shuff_rows['y_adj'].values
    df_syn_shuff.loc[:,['post_type']] = shuff_rows['post_type'].values

    df_edges_shuff=df_syn_shuff.drop(['Unnamed: 0','x','y','z','y_adj','structure'],axis=1).value_counts().reset_index(name='weight')

    result_,order = get_conditional_output(df_edges_shuff,normalize=True)
    
    # Append the result as a row to the result_df
    result_shuff.append(result_)

result_shuff = np.asarray(result_shuff)

u_mat = result_shuff.mean(axis=0)

std_mat = result_shuff.std(axis=0)





df_syn_data = deepcopy(df_syn)
mask = df_syn_data['pre_type'].isin(pre_groups) & df_syn_data['post_type'].isin(post_groups)  # Filter out rows with post_type not in post_types_order and pre_tyep not in pre_types_order
df_syn_data = df_syn_data[mask]

df_edges_data=df_syn_data.drop(['Unnamed: 0','x','y','z','y_adj','structure'],axis=1).value_counts().reset_index(name='weight')

result_data,order = get_conditional_output(df_edges_data,normalize=True)

# Calculate the z-scores
z_scores = (result_data - u_mat) / std_mat
z_scores[np.isclose(std_mat, 0)] = 0  # Replace z-scores with 0 where std is 0 # Handle cases where std_2d is zero to avoid division by zero


cond_input_mat = pd.DataFrame(z_scores,columns = order, index = order)



# Define the desired order
order = ['sg1','mg1','lg','sg2','mg2','lf','mli']

# Reorder rows and columns
df_reordered=cond_input_mat.reindex(index=order, columns=order)

# Ensure the color range is centered around 0
vmin = -max(abs(df_reordered.min().min()), abs(df_reordered.max().max()))
vmax = -vmin

sns.set_context("paper",font_scale=1)
hfig,ax = plt.subplots(1,figsize=(2,2))
sns.heatmap(df_reordered,
    cmap="RdBu_r",  # Diverging colormap from red to blue
    vmin=vmin,
    vmax=vmax,
    center=0)

plt.savefig('/Users/kperks/Downloads/mli_output_conditional_output_analysis.svg', format='svg', transparent=True)


df_reordered



sns.clustermap(data=df_reordered,
    cmap="RdBu_r",  # Diverging colormap from red to blue
    vmin=vmin,
    vmax=vmax,
    center=0,figsize=(4,4))






df_syn_shuff = deepcopy(df_syn)
mask = df_syn_shuff['pre_type'].isin(pre_groups) & df_syn_shuff['post_type'].isin(post_groups)  # Filter out rows with post_type not in post_types_order and pre_tyep not in pre_types_order
df_syn_shuff = df_syn_shuff[mask]
# Shuffle the dataframe
# df_syn_rand.loc[:,['pre']] = df_syn_rand['pre'].sample(frac = 1).values ## *** this does not work unless you re-type the pre_type column after***
shuff_rows = df_syn_shuff[['pre','x','y','z','y_adj','pre_type']].sample(frac = 1)
df_syn_shuff.loc[:,['pre']] = shuff_rows['pre'].values
df_syn_shuff.loc[:,['x']] = shuff_rows['x'].values
df_syn_shuff.loc[:,['y']] = shuff_rows['y'].values
df_syn_shuff.loc[:,['z']] = shuff_rows['z'].values
df_syn_shuff.loc[:,['y_adj']] = shuff_rows['y_adj'].values
df_syn_shuff.loc[:,['pre_type']] = shuff_rows['pre_type'].values

df_edges_shuff=df_syn_shuff.drop(['Unnamed: 0','x','y','z','y_adj','structure'],axis=1).value_counts().reset_index(name='weight')

result_,order = get_conditional_output(df_edges_shuff)

cond_input_mat = pd.DataFrame(result_,columns = order, index = order)

# Define the desired order
order = ['sg1','mg1','lg','sg2','mg2','lf','mli']

# Reorder rows and columns
df_reordered=cond_input_mat.reindex(index=order, columns=order)

# Ensure the color range is centered around 0
vmin = -max(abs(df_reordered.min().min()), abs(df_reordered.max().max()))
vmax = -vmin

sns.set_context("paper",font_scale=1)
hfig,ax = plt.subplots(1,figsize=(2,2))
sns.heatmap(df_reordered,
    cmap="RdBu_r",  # Diverging colormap from red to blue
    vmin=vmin,
    vmax=vmax,
    center=0)





df_syn = pd.read_csv(dirpath / 'graphs/df_presyn_mli.csv')
syn = 'pre-synaptic'


len(df_syn)


df_syn = df_syn.rename(columns={'pre':'post','post':'pre','pre_type':'post_type','post_type':'pre_type'})
# df_syn


y_adj_col = []
for i,r in df_syn.iterrows():
    yoffset = func((r['x'], r['z']), *popt)
    y_adj = (r['y'] - yoffset)
    y_adj_col.append(y_adj)

df_syn.loc[:,'y_adj']=y_adj_col

for v in ['x','y','z','y_adj']:
    df_syn[v] = df_syn[v]/1000
df_syn['y_adj'] = df_syn['y_adj']*-1    





for i,r in df_syn.iterrows():
    try:
        df_syn.loc[i,'pre_type'] =df_type[df_type['id'].isin([r['pre']])].cell_type.values[0]
        df_syn.loc[i,'post_type']=df_type[df_type['id'].isin([r['post']])].cell_type.values[0]
    except:
        print(r['pre'], r['post'])
    continue

    # try:
    #     df_syn.loc[i,'post_type']=df_type[df_type['id'].isin([r['post']])].cell_type.values[0]
    # except:
    #     print(r['post'])
    # continue


# df_syn.loc[:,'post_type'] = [t.lower() for t in df_syn['post_type']]
# df_syn.loc[:,'pre_type'] = [t.lower() for t in df_syn['pre_type']]


# df_syn


df_syn[df_syn['post'].isin([365756203])]['pre'].unique()





df_syn.head()


# remove structure and soma diameter information if want to
# df_syn = df_syn.drop(['Unnamed: 0','x','y','z','structure'],axis=1)#,'pre_diam','post_diam','diam_diff'],axis=1) # 


df_edges=df_syn.drop(['Unnamed: 0','x','y','z','y_adj','structure'],axis=1).value_counts().reset_index(name='weight')





count_type = 'nsyn' # 'ncells' #
mask = (df_edges['pre_type'].isin(df_edges['pre_type'].unique()))#['aff','grc','smpl','sg1','sg2','mli','tsd','dml'])) # df_edges['pre'].isin([290552453,27220895,31694533,102463116,188296613,15401313,17877032,187151336,117041378,122039969,36165549]) & 
#[295969348,295969442,295969134,295969355,295968777,282228761,283375247, 283391297,283390956,282230475,268614458,268614383,273086215,187230424]

types_ = ['mli']#, 
df_map = pd.DataFrame()
for t in types_:
    if count_type == 'nsyn':
        df_grouped = df_edges[(df_edges['post_type']==t) & mask].groupby(
            ['post','post_type','pre_type']).sum(numeric_only=True).reset_index().pivot(
            index='post', columns='pre_type', values='weight').fillna(0).reset_index()
    
    if count_type == 'ncells':
        df_grouped = df_edges[(df_edges['pre_type']==t) & mask].groupby(
            ['pre','pre_type','post_type']).count().reset_index().pivot(
            index='pre', columns='post_type', values='post').fillna(0).reset_index()
    
    df_grouped['post_type']=t
    df_map = pd.concat([df_map,df_grouped])
    
df_map = df_map.fillna(0)
df_map = df_map.set_index('post')
df_map = df_map.drop(['post_type'],axis=1)


df_map.head()





target_order = ['aff','smpl','sg1','grc','sg2']#['lg','mg1','lf','mg2']#['lg','mg1','sg1','lf','mg2','sg2']#
df_map = df_map.loc[:,[t for t in target_order if t in df_map.columns.values]]

cmap = sns.color_palette("YlGnBu", as_cmap=True)  # Base colormap
cmap.set_under('white')  # Set color for values below the colormap's minimum (i.e., 0)

# Ensure all 0s are treated as "under" the colormap range
# df_map = df_map.replace(0, -1)  # Replace 0s with a value below the colormap range


# fig, axes = plt.subplots(1, figsize=(5,10))
# sns.heatmap(data=df_map.sort_values(['lg']), annot=False, fmt="0.0f", cmap="YlGnBu", ax=axes)#  .sort_index()
# # sns.heatmap(data=df_map.div(df_map.sum(axis=1),axis=0).mul(100).round(0), annot=True, fmt="0.0f", cmap="YlGnBu", ax=axes[1])
# axes.set_title('total syn per pre cell')




# Plot the heatmap
fig, axes = plt.subplots(1, figsize=(5,10))
sns.heatmap(data=df_map.sort_values(['sg1']), annot=False, fmt="0.0f", cmap=cmap, 
            ax=axes, vmin=1)  # vmin slightly above 0 to treat negative as "under"
axes.set_title('total syn per post cell')
axes.set_yticks([])




sg1_map = df_map[df_map['sg1']>1].sort_values(['sg1'])


sg2_map = df_map[~df_map.index.isin(sg1_map.index)].sort_values(['sg2'],ascending=False)


sorted_full = pd.concat([sg2_map,sg1_map])


# Plot the heatmap
fig, axes = plt.subplots(1, figsize=(5,10))
sns.heatmap(data=sorted_full, annot=False, fmt="0.0f", cmap=cmap, 
            ax=axes, vmin=1)  # vmin slightly above 0 to treat negative as "under"
axes.set_title('total syn per pre cell')
axes.set_yticks([])


# Plotting
fig, ax = plt.subplots(figsize=(5, 5))
# Create a stacked bar plot
sorted_full.loc[:,target_order].plot.bar(stacked=True,color = syn_colors,ax=ax, width=0.9)
ax.set_xticks([])
ax.legend(title='Postsynaptic \n Type', bbox_to_anchor=(1.4, 1), loc='upper right')
plt.savefig('/Users/kperks/Downloads/output_stacked_bar_presyn.svg', format='svg', transparent=True)


df_norm = df_map.div(df_map.sum(axis=1),axis=0).mul(100)#.round(0)


sg1_map = df_norm[df_norm['sg1']>1].sort_values(['sg1'])

sg2_map = df_norm[~df_norm.index.isin(sg1_map.index)].sort_values(['sg2'],ascending=False)

norm_sorted_full = pd.concat([sg2_map,sg1_map])


# Plot the heatmap
fig, axes = plt.subplots(1, figsize=(5,10))
sns.heatmap(data=norm_sorted_full, annot=False, fmt="0.0f", cmap=cmap, 
            ax=axes, vmin=1)  # vmin slightly above 0 to treat negative as "under"
axes.set_title('total syn per pre cell')
axes.set_yticks([])


# Plotting
fig, ax = plt.subplots(figsize=(15, 5))
# Create a stacked bar plot
norm_sorted_full.loc[:,target_order].plot.bar(stacked=True,color = syn_colors,ax=ax, width=0.9)
ax.set_xticks([])
ax.legend(title='Postsynaptic \n Type', bbox_to_anchor=(1.15, 1), loc='upper right')
plt.savefig('/Users/kperks/Downloads/mg_output_stacked_bar_presyn_normalized.svg', format='svg', transparent=True)








directory_path_mark = Path('/Users/kperks/Library/CloudStorage/GoogleDrive-sawtelllab@gmail.com/My Drive/ELL_connectome/CREST_reconstructions/Spine_Density_Annotation/from_mark')


cell_filepaths_mark = get_cell_filepaths(directory_path_mark) # gets filepaths for all cells in a directory"
cell_filepaths_mg_network = get_cell_filepaths(Path('/Users/kperks/Library/CloudStorage/GoogleDrive-sawtelllab@gmail.com/My Drive/ELL_connectome/CREST_reconstructions/mg-network')) # gets filepaths for all cells in a directory"


cellids = list(cell_filepaths_mark.keys())


cellids


df_type[df_type['id'].isin([int('128737253')])]['cell_type'].values[0] in ['lg','lf']


[c for c in cellids if df_type[df_type['id'].isin([int(c)])]['cell_type'].values[0] in ['lg','lf']] #['mg1','mg2']]



id_ = '128737253'
cell = ecrest(settings_dict,filepath = cell_filepaths_main[id_], launch_viewer=True)


anno_loc = 'spineD loc'
anno_pts = 'spineD pts'
vx_sizes = [16, 16, 30]
'''assumes that the annotation is a point annotation stored in the list as ([x,y,z,segment_id],'annotatePoint')
                previous ot Jan 25 2024, it was just [x,y,z,segment_id]'''

data = []

for id_ in cellids: #[c for c in cellids if df_type[df_type['id'].isin([int(c)])]['cell_type'].values[0] in ['mg1','mg2']]:# ['lg','lf']]: #
    # id_ = cellids[0]
    
    # open cell from mg_network for cell type because got deleted from some of Marks files?
    cell = ecrest(settings_dict,filepath = cell_filepaths_mg_network[id_],launch_viewer=False)
    ctype = cell.get_ctype("manual")
    
    # open cell from mark's folder to get spine annotation data
    cell = ecrest(settings_dict,filepath = cell_filepaths_mark[id_],launch_viewer=False)
        
    for loc_ in cell.cell_data['end_points'][anno_loc]:
        x,y,z = [p for p in loc_[0]]     
        y = -(y/16 - 16210)
        x = x/16
        z = z/30
    
        d = [np.linalg.norm(np.array(pt_[0]) - np.array(loc_[0])) for pt_ in cell.cell_data['end_points'][anno_pts] if (np.linalg.norm(np.array(pt_[0]) - np.array(loc_[0])))<5100]
        
        data.append({'id': id_, 'cell_type': ctype[0], 'x': x, 'y': y, 'z': z, 'n': len(d)})#, ignore_index=True)

df = pd.DataFrame(data)


df.sort_values('x')


ax = sns.scatterplot(data=df,x='x',y='y',size='n',hue='cell_type',sizes=(2, 200),legend=True)
# ax.set_aspect('equal', adjustable='datalim')
# ax.set_ylim(0,300)
ax.set_xlim(0,30000)
# for y_ in [2365*16, 4344*16, 8432*16, 11138*16, 13021*16, 15045*16, 15700*16]:
#     ax.axhline(y=y_/1000,color = 'black',linestyle='--')
# ax.invert_yaxis()



ax = sns.scatterplot(data=df,x='n',y='y',hue='cell_type',legend=True)


# Define depth bins and labels
bins = [1000, 3000, 5000, 7000, 9000, 12000,14000]
labels = ['d0', 'd1', 'd2', 'd3', 'd4','d5']

# Create a new column for the depth bin based on 'y'
df['depth_bin'] = pd.cut(df['y'], bins=bins, labels=labels, right=False)

# Calculate the mean 'n' for each depth bin
mean_n_per_bin = df.groupby(['depth_bin','cell_type'], observed=False)['n'].mean()


df


sns.boxplot(data = df,x = 'depth_bin', y = 'n',hue='cell_type')


df.groupby(['depth_bin','cell_type'])['n'].mean()


density_mg = [53.000000,95.647059,113.352941,113.388889,88.187500,50.958333]/10 #spines per annotation sphere divided by diameter annotation sphere
density_sg = [20,20,20,20,20,20]/10
density_out = [38.647059,55.360000,64.944444,74.555556,63.800000,70.000000]/10


df = pd.DataFrame(mean_n_per_bin).reset_index().rename(columns={'depth_bin':'label'})#(names='label')





jsondir = Path('/Users/kperks/Library/CloudStorage/GoogleDrive-sawtelllab@gmail.com/My Drive/ELL_connectome/CREST_reconstructions/Spine_Density_Annotation/dend_length')
flist = ['299496636_mg1.json','214581797_mg2.json','301787806_lg.json','393325331_lf.json'] #

df_summary = pd.DataFrame(columns=['label','length','cell_type'])

for fname in flist:
    row_dict = {}
    with open(jsondir / fname, 'r') as myfile: # 'p' is the dirpath and 'f' is the filename from the created 'd' dictionary
        json_data = json.load(myfile)


    data = {}
    dlist = ['d0','d1','d2','d3','d4','d5','d6']
    llist = [2000,2000,2000,2000,2000,2000,2000]
    for i,d in enumerate(dlist[:-1]):
        d1 = len(next((item for item in json_data['layers'] if item["name"] == dlist[i]), None)['annotations'])
        d2 = len(next((item for item in json_data['layers'] if item["name"] == dlist[i+1]), None)['annotations'])
        # print(d1,d2)
        row_dict={
            'label' : [d],
            'length' : [np.mean([d1,d2]) * llist[i] * 16 /1000], # total length in bin = number branches times llist[i] voxels long, 16nm per voxel, 1micron per 1000 nm
            'cell_type' : [fname.split('_')[1].split('.')[0][0]],
            'id' : [fname.split('_')[0]]
        }

        df_summary = pd.concat([df_summary,pd.DataFrame.from_dict(row_dict,orient = 'columns')])
    # df_summary = pd.concat([df_summary,pd.DataFrame.from_dict(data,orient='index').reset_index(names = 'label').rename(columns={0:'length'})],
    #          ignore_index=True)


df_summary
sns.boxplot(data = df_summary,x = 'label', y = 'length',hue='cell_type')


mean_l_per_bin = df_summary.groupby(['label','cell_type'], observed=False)['length'].mean()


df_summary = pd.DataFrame(mean_l_per_bin).reset_index()#(names='label')


df_summary





df_summary = pd.merge(df, df_summary, on=['label','cell_type'])


df_summary






for i,r in df_summary.iterrows():
    df_summary.loc[i,'total']=(r['n']/10)*r['length']


df_summary


df_summary.groupby(['cell_type'])['total'].sum()


# hfig,ax = plt.subplots(figsize=4,4)
h = sns.scatterplot(data = df_summary,x = 'label', y = 'total',hue='cell_type',s=200, palette = {'l':'orange','m':'blue'})
# ax = h.gca()
h.set_xticklabels(np.asarray([3000,5000,7000,9000,11000,13000])*16/1000);
h.set_xlabel('microns up molecular layer')
h.set_ylabel('spines per depth bin')


h








def get_p_connect(df_edges,mask,count_type):

# count_type = 'nsyn' # 'ncells' #
# mask = (df_edges['post_type'].isin(['sg1','sg2','mg1','mg2','lg','lf']))#,'mli','tsd','dml'])) # df_edges['pre'].isin([290552453,27220895,31694533,102463116,188296613,15401313,17877032,187151336,117041378,122039969,36165549]) & 
#[295969348,295969442,295969134,295969355,295968777,282228761,283375247, 283391297,283390956,282230475,268614458,268614383,273086215,187230424]

    types_ = ['pf'] #['mg1','mg2']#should be all that is in df_pfsyn anyway
    df_map = pd.DataFrame()
    for t in types_:
        if count_type == 'nsyn':
            df_grouped = df_edges[(df_edges['pre_type']==t) & mask].groupby(
                ['pre','pre_type','post_type']).sum(numeric_only=True).reset_index().pivot(
                index='pre', columns='post_type', values='weight').fillna(0).reset_index()
        
        if count_type == 'ncells':
            df_grouped = df_edges[(df_edges['pre_type']==t) & mask].groupby(
                ['pre','pre_type','post_type']).count().reset_index().pivot(
                index='pre', columns='post_type', values='post').fillna(0).reset_index()
        
        df_grouped['pre_type']=t
        df_map = pd.concat([df_map,df_grouped])
        
    df_map = df_map.fillna(0)
    df_map = df_map.set_index('pre')
    df_map = df_map.drop(['pre_type'],axis=1)
    
    
    df_norm = df_map.div(df_map.sum(axis=1),axis=0)#.mul(100).round(0)

    return df_norm
# sns.heatmap(data=df_norm.sort_values('lcf'), annot=True, fmt="0.0f", cmap="YlGnBu")





df_syn = pd.read_csv(dirpath / 'graphs/df_pfsyn.csv')
syn = 'post-synaptic'


for i,r in df_syn.iterrows():
    try:
        df_syn.loc[i,'pre_type'] =df_type[df_type['id'].isin([r['pre']])].cell_type.values[0]
        df_syn.loc[i,'post_type']=df_type[df_type['id'].isin([r['post']])].cell_type.values[0]
    except:
        print(r['pre'],r['post'])
        continue

df_syn.loc[:,'post_type'] = [t.lower() for t in df_syn['post_type']]
df_syn.loc[:,'pre_type'] = [t.lower() for t in df_syn['pre_type']]


# include only connections to the following types/groups of cells
included_groups = ['lg','mg1','sg1','lf','mg2','sg2']
df_syn = df_syn[(df_syn['post_type'].isin(included_groups))]#& df_syn['pre_type'].isin(['mg1','mg2'])]
len(df_syn)


df_syn['pre'].nunique()


df_syn.head()





df_syn_rand = deepcopy(df_syn)





count_type = 'nsyn'
by_group='lf'





df_edges=df_syn.drop(['Unnamed: 0','x','y','z','structure'],axis=1).value_counts().reset_index(name='weight')

mask = (df_edges['post_type'].isin(included_groups))

df_norm_data = get_p_connect(df_edges,mask,count_type)

result_data = df_norm_data[(df_norm_data[by_group] != 0)].mean()





result_df = pd.DataFrame()

# Iterate the specified number of times
for i in range(100):
    # Shuffle the dataframe
    # df_syn_rand.loc[:,['pre']] = df_syn_rand['pre'].sample(frac = 1).values ## *** this does not work unless you re-type the pre_type column after***
    shuff_rows = df_syn_rand[['pre','pre_type']].sample(frac = 1)
    df_syn_rand.loc[:,['pre']] = shuff_rows['pre'].values
    df_syn_rand.loc[:,['pre_type']] = shuff_rows['pre_type'].values

    df_edges=df_syn_rand.drop(['Unnamed: 0','x','y','z','structure'],axis=1).value_counts().reset_index(name='weight')
    mask = (df_edges['post_type'].isin(included_groups))

    df_norm = get_p_connect(df_edges,mask,count_type)
    
    # Compute the mean for rows where 'mg2' is not zero
    mean_row = df_norm[(df_norm[by_group] != 0)].mean(numeric_only=True) #df_shuffled[df_shuffled[by_group] != 0].mean(numeric_only=True)
    
    # Append the result as a row to the result_df
    result_df = pd.concat([result_df, pd.DataFrame([mean_row])], ignore_index=True)





hfig,ax = plt.subplots(1,figsize=(2,3))

# plot single iteration, individual cells
# sns.pointplot(data = df_norm,linestyle='',color = 'black',markersize = 5,order = ['lg','mg1','sg1','lf','mg2','sg2'],errorbar='sd')
# sns.pointplot(data=df_norm_data[(df_norm_data[by_group] != 0)],linestyle='',color = 'red',markersize = 5,order = ['lg','mg1','sg1','lf','mg2','sg2'],errorbar='sd')

# plot iteration means
sns.pointplot(data = result_df,linestyle='',color = 'black',markersize = 3,order = included_groups,errorbar=('ci'))
sns.pointplot(data=result_data,linestyle='',color = 'red',markersize = 3,order = included_groups)

ax.set_ylabel('fraction connections')
ax.set_title(f'by group {by_group}')

plt.savefig(f'/Users/kperks/Downloads/pf_output_conditionalP_{by_group}.svg', format='svg', transparent=True)



# sns.set(font_scale=1.4)
sns.clustermap(data=df_map,z_score=0,cmap='vlag',col_cluster=False, vmin=-1.5, vmax=1.5,figsize=(5,8))


df_map[(df_map['lg'] != 0)& (df_map['lf'] != 0)]


hfig,ax = plt.subplots(3,2,figsize=(8,10))
sns.scatterplot(df_map[(df_map['lg'] != 0)& (df_map['lf'] != 0)], x = 'lf', y = 'lg', ax = ax[0][0],alpha = 0.25,color = 'black')
sns.scatterplot(df_map[(df_map['mg1'] != 0)& (df_map['mg2'] != 0)], x = 'mg1', y = 'mg2', ax = ax[0][1],alpha = 0.25,color = 'black')
sns.scatterplot(df_map[(df_map['lg'] != 0)& (df_map['mg1'] != 0)], x = 'lg', y = 'mg1', ax = ax[1][0],alpha = 0.25,color = 'black')
sns.scatterplot(df_map[(df_map['lf'] != 0)& (df_map['mg2'] != 0)], x = 'lf', y = 'mg2', ax = ax[1][1],alpha = 0.25,color = 'black')
sns.scatterplot(df_map[(df_map['lg'] != 0)& (df_map['mg2'] != 0)], x = 'lg', y = 'mg2', ax = ax[2][0],alpha = 0.25,color = 'black')
sns.scatterplot(df_map[(df_map['lf'] != 0)& (df_map['mg1'] != 0)], x = 'lf', y = 'mg1', ax = ax[2][1],alpha = 0.25,color = 'black')
