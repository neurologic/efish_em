{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import json\n",
    "# Initialize a viewer with the base segmentation for the box of interest\n",
    "import neuroglancer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_next_element(viewer,neurites):\n",
    "    element = int(neurites[0])\n",
    "    \n",
    "    # Define color mapping for the segment(s)\n",
    "    color_value = \"#ef8200\"\n",
    "    segment_color_mapping = {element: color_value}\n",
    "    \n",
    "    # Update the viewer\n",
    "    with viewer.txn() as s:\n",
    "        # Assign segments using `set()`, since `.segments` is a Neuroglancer `VisibleSegments` object\n",
    "        s.layers['segmentation'].segments = set([element])\n",
    "        s.layers['segmentation'].segment_colors = segment_color_mapping\n",
    "        s.layers['segmentation'].visible = True\n",
    "    \n",
    "    print(f'this element is {element}')\n",
    "    print('Remaining neurites '+ str(len(neurites)))\n",
    "    \n",
    "    return element\n",
    "\n",
    "def query_save_and_reset_viewer(viewer,element,proofread_objects,filename_proofread_list,filename_ref_list):\n",
    "    ##################################################################################################\n",
    "    # SAVE\n",
    "    \n",
    "    segments = [int(s) for s in list((viewer.state.layers['segmentation'].segments))]\n",
    "    \n",
    "    if str(element) in list(proofread_objects.keys()):\n",
    "        print('ERROR! element already used as a key in the json dict')\n",
    "        # Grab the reference list segments (all segments in the box >1000 voxels)\n",
    "        with open(filename_ref_list, 'r') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            ref_list = [int(value) for row in reader for value in row]  # Flattened list of uint64 values\n",
    "            proofread_list = set([num for k,v in proofread_objects.items() for num in v])\n",
    "            neurites = list(set(ref_list).difference(proofread_list))\n",
    "        return proofread_objects, neurites\n",
    "        \n",
    "    elif str(element) not in list(proofread_objects.keys()):\n",
    "        print('this element does not exist -- updating proofread_elements and reseting viewer')\n",
    "        proofread_objects[element] = segments # key need s to be element, because element is not always segments[0] and then end up with duplicate keys in split_corrected\n",
    "           \n",
    "    with open(filename_proofread_list, 'w') as outfile:\n",
    "        # json.dump([{k:v} for k,v in proofread_objects.items()], outfile) #use this if want a list of dictionaries rather than a dictionary of all\n",
    "        json.dump(proofread_objects, outfile)\n",
    "    \n",
    "    ################################################################################################\n",
    "    # RESET \n",
    "    # Grab the reference list segments (all segments in the box >1000 voxels)\n",
    "    with open(filename_ref_list, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        ref_list = [int(value) for row in reader for value in row]  # Flattened list of uint64 values\n",
    "    \n",
    "    # Grab the split-corrected segments so far\n",
    "    if os.path.exists(filename_proofread_list):\n",
    "        with open(filename_proofread_list, 'r') as f:\n",
    "            proofread_objects = json.load(f)\n",
    "    else:\n",
    "        with open(filename_proofread_list, 'w') as outfile:  \n",
    "            json.dump([], outfile)\n",
    "        proofread_objects = {}\n",
    "    \n",
    "    # Extract proofread segment IDs (make it a flat list) and subtract it from the ref_list to get the remaining list\n",
    "    proofread_list = set([num for k,v in proofread_objects.items() for num in v])\n",
    "    neurites = list(set(ref_list).difference(proofread_list))\n",
    "    ntotal = len(neurites)\n",
    "    print('Remaining neurites '+ str(len(neurites)))\n",
    "    \n",
    "    # Remove all segments from the \"segmentation\" layer\n",
    "    with viewer.txn() as txn:\n",
    "        layer = txn.layers['segmentation']\n",
    "        layer.segments = set()  # Assign an empty set to clear the segments\n",
    "    \n",
    "    with viewer.txn(overwrite=True) as s:\n",
    "        s.layers['Base Segment Merger'] = neuroglancer.AnnotationLayer()\n",
    "        s.layers['Base Segment Merger'].filterBySegmentation = [\"segments\"]\n",
    "        s.layers['Base Segment Merger'].linkedSegmentationLayer = {\"segments\": 'segmentation'}\n",
    "        s.layers['Base Segment Merger'].annotationColor = '#ffa500'\n",
    "        s.layers['Base Segment Merger'].tool = \"annotatePoint\"\n",
    "        \n",
    "    \n",
    "    point_type = 'ends'\n",
    "    with viewer.txn(overwrite=True) as s:\n",
    "        s.layers[point_type] = neuroglancer.AnnotationLayer()\n",
    "        s.layers[point_type].annotationColor = '#ffff00'\n",
    "        s.layers[point_type].tool = \"annotatePoint\"\n",
    "        s.layers[point_type].tab = 'Annotations'\n",
    "    \n",
    "    # RUN STEP1 again\n",
    "    \n",
    "    return proofread_objects, neurites\n",
    "\n",
    "def save_and_reset_viewer(viewer,proofread_objects,filename_proofread_list,filename_ref_list):\n",
    "    ##################################################################################################\n",
    "    # SAVE\n",
    "\n",
    "    with open(filename_proofread_list, 'w') as outfile:\n",
    "        # json.dump([{k:v} for k,v in proofread_objects.items()], outfile) #use this if want a list of dictionaries rather than a dictionary of all\n",
    "        json.dump(proofread_objects, outfile)\n",
    "    \n",
    "    ################################################################################################\n",
    "    # RESET \n",
    "    # Grab the reference list segments (all segments in the box >1000 voxels)\n",
    "    with open(filename_ref_list, 'r') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        ref_list = [int(value) for row in reader for value in row]  # Flattened list of uint64 values\n",
    "    \n",
    "    # Grab the split-corrected segments so far\n",
    "    if os.path.exists(filename_proofread_list):\n",
    "        with open(filename_proofread_list, 'r') as f:\n",
    "            proofread_objects = json.load(f)\n",
    "    else:\n",
    "        with open(filename_proofread_list, 'w') as outfile:  \n",
    "            json.dump([], outfile)\n",
    "        proofread_objects = {}\n",
    "    \n",
    "    # Extract proofread segment IDs (make it a flat list) and subtract it from the ref_list to get the remaining list\n",
    "    proofread_list = set([num for k,v in proofread_objects.items() for num in v])\n",
    "    neurites = list(set(ref_list).difference(proofread_list))\n",
    "    ntotal = len(neurites)\n",
    "    print('Remaining neurites '+ str(len(neurites)))\n",
    "    \n",
    "    # Remove all segments from the \"segmentation\" layer\n",
    "    with viewer.txn() as txn:\n",
    "        layer = txn.layers['segmentation']\n",
    "        layer.segments = set()  # Assign an empty set to clear the segments\n",
    "    \n",
    "    with viewer.txn(overwrite=True) as s:\n",
    "        s.layers['Base Segment Merger'] = neuroglancer.AnnotationLayer()\n",
    "        s.layers['Base Segment Merger'].filterBySegmentation = [\"segments\"]\n",
    "        s.layers['Base Segment Merger'].linkedSegmentationLayer = {\"segments\": 'segmentation'}\n",
    "        s.layers['Base Segment Merger'].annotationColor = '#ffa500'\n",
    "        s.layers['Base Segment Merger'].tool = \"annotatePoint\"\n",
    "        \n",
    "    \n",
    "    point_type = 'ends'\n",
    "    with viewer.txn(overwrite=True) as s:\n",
    "        s.layers[point_type] = neuroglancer.AnnotationLayer()\n",
    "        s.layers[point_type].annotationColor = '#ffff00'\n",
    "        s.layers[point_type].tool = \"annotatePoint\"\n",
    "        s.layers[point_type].tab = 'Annotations'\n",
    "    \n",
    "    # RUN STEP1 again\n",
    "    \n",
    "    return proofread_objects, neurites\n",
    "    \n",
    "def check_against_proofread(segments,proofread_objects):\n",
    "    overlap = []\n",
    "    num_dup = []\n",
    "    for k,v in proofread_objects.items():\n",
    "        overlap.append(len(set(self_segments)&set(v))/len(set(v)))\n",
    "        num_dup.append(len(set(self_segments)&set(v)))\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"dups\": list(proofread_objects.keys()),\n",
    "        \"overlap-percent\": overlap,\n",
    "        \"number_seg_lap\": num_dup\n",
    "        }).replace(0, np.nan, inplace=False).dropna()\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining neurites 350\n"
     ]
    }
   ],
   "source": [
    "# INITIALIZE #\n",
    "dirpath = '/Users/kperks/Library/CloudStorage/GoogleDrive-sawtelllab@gmail.com/My Drive/ELL_connectome/MolecularLayerModel_Michal/Efish_proofreading_box1/'\n",
    "filename_ref_list = dirpath + \"efish_box1_segments_list.csv\"\n",
    "# filename_proofread_list = 'box1_split_corrected.json'\n",
    "filename_proofread_list = dirpath + 'box1_split_corrected_dict.json'\n",
    "\n",
    "# Grab the reference list segments (all segments in the box >1000 voxels)\n",
    "with open(filename_ref_list, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    ref_list = [int(value) for row in reader for value in row]  # Flattened list of uint64 values\n",
    "\n",
    "# Grab the split-corrected segments so far\n",
    "if os.path.exists(filename_proofread_list):\n",
    "    with open(filename_proofread_list, 'r') as f:\n",
    "        proofread_objects = json.load(f)\n",
    "else:\n",
    "    with open(filename_proofread_list, 'w') as outfile:  \n",
    "        json.dump([], outfile)\n",
    "    proofread_objects = []\n",
    "\n",
    "# Extract proofread segment IDs (make it a flat list) and subtract it from the ref_list to get the remaining list\n",
    "proofread_list = set([num for k,v in proofread_objects.items() for num in v])\n",
    "remaining_list = list(set(ref_list).difference(proofread_list))\n",
    "neurites = remaining_list\n",
    "ntotal = len(neurites)\n",
    "print('Remaining neurites '+ str(len(neurites)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://127.0.0.1:53629/v/db8cd997fc34f7c4b4785424f2498c13d2e97d88/\" target=\"_blank\">Viewer</a>"
      ],
      "text/plain": [
       "http://127.0.0.1:53629/v/db8cd997fc34f7c4b4785424f2498c13d2e97d88/"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = neuroglancer.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    s.layers['image'] = neuroglancer.ImageLayer(source='brainmaps://10393113184:ell:roi450um_xyz')\n",
    "    s.layers['segmentation'] = neuroglancer.SegmentationLayer(\n",
    "        source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "        selected_alpha=0.5\n",
    "    )\n",
    "\n",
    "# Initialize a neurites counter\n",
    "viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\n",
    "  \"type\": \"annotation\",\n",
    "  \"source\": {\n",
    "    \"url\": \"local://annotations\",\n",
    "    \"transform\": {\n",
    "      \"outputDimensions\": {\n",
    "        \"x\": [\n",
    "          1.6e-8,\n",
    "          \"m\"\n",
    "        ],\n",
    "        \"y\": [\n",
    "          1.6e-8,\n",
    "          \"m\"\n",
    "        ],\n",
    "        \"z\": [\n",
    "          3e-8,\n",
    "          \"m\"\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"tool\": \"annotateBoundingBox\",\n",
    "  \"tab\": \"annotations\",\n",
    "  \"annotations\": [\n",
    "    {\n",
    "      \"pointA\": [\n",
    "        15075,\n",
    "        10620,\n",
    "        1618\n",
    "      ],\n",
    "      \"pointB\": [\n",
    "        15700,\n",
    "        11245,\n",
    "        1951\n",
    "      ],\n",
    "      \"type\": \"axis_aligned_bounding_box\",\n",
    "      \"id\": \"8470042ac60231441cf2de9988aabfc42cfe1965\"\n",
    "    }\n",
    "  ],\n",
    "  \"name\": \"box1\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with viewer.txn(overwrite=True) as s:\n",
    "\n",
    "    s.layers['Base Segment Merger'] = neuroglancer.AnnotationLayer()\n",
    "    s.layers['Base Segment Merger'].filterBySegmentation = [\"segments\"]\n",
    "    s.layers['Base Segment Merger'].linkedSegmentationLayer = {\"segments\": 'segmentation'}\n",
    "    s.layers['Base Segment Merger'].annotationColor = '#ffa500'\n",
    "    s.layers['Base Segment Merger'].tool = \"annotatePoint\"\n",
    "    \n",
    "\n",
    "point_type = 'ends'\n",
    "with viewer.txn(overwrite=True) as s:\n",
    "    s.layers[point_type] = neuroglancer.AnnotationLayer()\n",
    "    s.layers[point_type].annotationColor = '#ffff00'\n",
    "    s.layers[point_type].tool = \"annotatePoint\"\n",
    "    s.layers[point_type].tab = 'Annotations'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this element is 286782393\n",
      "Remaining neurites 334\n"
     ]
    }
   ],
   "source": [
    "# Load the first element of the neurites list and make sure it is an int\n",
    "element = load_next_element(viewer,neurites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dups</th>\n",
       "      <th>overlap-percent</th>\n",
       "      <th>number_seg_lap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [dups, overlap-percent, number_seg_lap]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check to see if duplicate with any already read proofread objects\n",
    "self_segments = [int(s) for s in list((viewer.state.layers['segmentation'].segments))]\n",
    "df_dups = check_against_proofread(self_segments,proofread_objects)\n",
    "display(df_dups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this element does not exist -- updating proofread_elements and reseting viewer\n",
      "Remaining neurites 334\n"
     ]
    }
   ],
   "source": [
    "proofread_objects, neurites = query_save_and_reset_viewer(viewer,element,proofread_objects,filename_proofread_list,filename_ref_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# consolidate with existing duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## if you are sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dupid = '286781626'\n",
    "consolidated_list = set(proofread_objects[dupid] + self_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "proofread_objects[dupid] = list(consolidated_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining neurites 342\n"
     ]
    }
   ],
   "source": [
    "proofread_objects, neurites = save_and_reset_viewer(viewer,proofread_objects,filename_proofread_list,filename_ref_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## if you want to check visually first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the viewer\n",
    "lname = 'self'\n",
    "segment_color_mapping = {key: \"#ef8200\" for key in self_segments}\n",
    "with viewer2.txn() as s:\n",
    "    s.layers[lname] = neuroglancer.SegmentationLayer(\n",
    "        source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "        selected_alpha=0.5)\n",
    "    # Assign segments using `set()`, since `.segments` is a Neuroglancer `VisibleSegments` object\n",
    "    s.layers[lname].segments = set(self_segments)\n",
    "    s.layers[lname].segment_colors = segment_color_mapping\n",
    "    s.layers[lname].visible = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "for d in df_dups['dups'].values:\n",
    "    segs = list(set(proofread_objects[d]))\n",
    "    # color_value = generate_random_hex_color()# \"#ef8200\"\n",
    "    # segment_color_mapping = {key: generate_random_hex_color() for key in segs}\n",
    "    \n",
    "    \n",
    "    # Update the viewer\n",
    "    lname = d\n",
    "    with viewer2.txn() as s:\n",
    "        s.layers[lname] = neuroglancer.SegmentationLayer(\n",
    "            source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "            selected_alpha=0.5)\n",
    "        # Assign segments using `set()`, since `.segments` is a Neuroglancer `VisibleSegments` object\n",
    "        s.layers[lname].segments = set(segs)\n",
    "        # s.layers[lname].segment_colors = segment_color_mapping\n",
    "        s.layers[lname].visible = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dupid = '286797753'\n",
    "consolidated_list = set(proofread_objects[dupid] + self_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lname = 'consolidated'\n",
    "segment_color_mapping = {key: \"#ef8200\" for key in self_segments}\n",
    "with viewer2.txn() as s:\n",
    "    s.layers[lname] = neuroglancer.SegmentationLayer(\n",
    "        source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "        selected_alpha=0.5)\n",
    "    # Assign segments using `set()`, since `.segments` is a Neuroglancer `VisibleSegments` object\n",
    "    s.layers[lname].segments = set(consolidated_list)\n",
    "    # s.layers[lname].segment_colors = segment_color_mapping\n",
    "    s.layers[lname].visible = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "proofread_objects[dupid] = list(consolidated_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining neurites 1909\n"
     ]
    }
   ],
   "source": [
    "proofread_objects, neurites = save_and_reset_viewer(viewer,proofread_objects,filename_proofread_list,filename_ref_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename_proofread_list, 'r') as f:\n",
    "    proofread_objects = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicates(base_segments):\n",
    "    '''\n",
    "    base_segments is a dictionary of all segments that this script checks among\n",
    "    '''\n",
    "    df_all = pd.DataFrame()\n",
    "    dups_list = []\n",
    "    for self_k,this_cell in base_segments.items():\n",
    "        # print(dups_list)\n",
    "        if self_k not in dups_list:\n",
    "            overlap = []\n",
    "            num_dup = []\n",
    "            for x in base_segments.keys():\n",
    "                overlap.append(len(set(this_cell)&set(base_segments[x]))/len(set(base_segments[x])))\n",
    "                num_dup.append(len(set(this_cell)&set(base_segments[x])))\n",
    "    \n",
    "            df = pd.DataFrame({\n",
    "                \"self\": self_k,\n",
    "                \"dups\": list(base_segments.keys()),\n",
    "                \"overlap-percent\": overlap,\n",
    "                \"number_seg_lap\": num_dup\n",
    "                }).replace(0, np.nan, inplace=False).dropna()\n",
    "            df = df[df['dups'] != self_k]\n",
    "            \n",
    "            if not df.empty:\n",
    "                dups_list.extend([v for v in df['dups'].values])\n",
    "                df_all = pd.concat([df_all,df]) \n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://127.0.0.1:53629/v/325ba2e64258d30f78e498f879387d41c5654dc7/\" target=\"_blank\">Viewer</a>"
      ],
      "text/plain": [
       "http://127.0.0.1:53629/v/325ba2e64258d30f78e498f879387d41c5654dc7/"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer2 = neuroglancer.Viewer()\n",
    "with viewer2.txn() as s:\n",
    "    s.layers['image'] = neuroglancer.ImageLayer(source='brainmaps://10393113184:ell:roi450um_xyz')\n",
    "\n",
    "# Initialize a neurites counter\n",
    "viewer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_dict = check_duplicates(proofread_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>self</th>\n",
       "      <th>dups</th>\n",
       "      <th>overlap-percent</th>\n",
       "      <th>number_seg_lap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>286796800</td>\n",
       "      <td>286796414</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>371507331</td>\n",
       "      <td>371508228</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>372667908</td>\n",
       "      <td>372654163</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>285652483</td>\n",
       "      <td>285637634</td>\n",
       "      <td>0.004329</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>286780872</td>\n",
       "      <td>286796588</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>371524290</td>\n",
       "      <td>286798108</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>371507421</td>\n",
       "      <td>285637634</td>\n",
       "      <td>0.004329</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>286781584</td>\n",
       "      <td>371507724</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          self       dups  overlap-percent  number_seg_lap\n",
       "702  286796800  286796414         0.222222             2.0\n",
       "596  371507331  371508228         0.400000             2.0\n",
       "412  372667908  372654163         0.200000             2.0\n",
       "25   285652483  285637634         0.004329             2.0\n",
       "741  286780872  286796588         0.500000             2.0\n",
       "935  371524290  286798108         0.750000             6.0\n",
       "25   371507421  285637634         0.004329             2.0\n",
       "166  286781584  371507724         0.012195             2.0"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_dict[dup_dict['number_seg_lap']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proofread_objects.pop('371523785')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_id = '371524290'\n",
    "dup_list = list(dup_dict[(dup_dict['self']==self_id) & (dup_dict['number_seg_lap']>1)]['dups'].values)\n",
    "\n",
    "for d in [self_id] + dup_list:\n",
    "    # Define color mapping for the segment(s)\n",
    "    segs = list(set(proofread_objects[d]))\n",
    "    # color_value = generate_random_hex_color()# \"#ef8200\"\n",
    "    # segment_color_mapping = {key: color_value for key in segs}\n",
    "\n",
    "\n",
    "    # Update the viewer\n",
    "    lname = d\n",
    "    with viewer2.txn() as s:\n",
    "        s.layers[lname] = neuroglancer.SegmentationLayer(\n",
    "            source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "            selected_alpha=0.5)\n",
    "        # Assign segments using `set()`, since `.segments` is a Neuroglancer `VisibleSegments` object\n",
    "        s.layers[lname].segments = segs\n",
    "        # s.layers[lname].segment_colors = segment_color_mapping\n",
    "        s.layers[lname].visible = True\n",
    "    \n",
    "    # print(f'this element is {element}')\n",
    "    # print('Remaining neurites '+ str(len(neurites)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_dict_consolidated = {}\n",
    "\n",
    "v_list = []\n",
    "for d in [self_id] + dup_list:\n",
    "    v_list.append(proofread_objects[d])\n",
    "dup_dict_consolidated[self_id] = list(set([num for ni in v_list for num in ni]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dup_dict_consolidated\n",
    "\n",
    "segs = dup_dict_consolidated[self_id]\n",
    "# color_value = generate_random_hex_color()# \"#ef8200\"\n",
    "# segment_color_mapping = {key: color_value for key in segs}\n",
    "\n",
    "\n",
    "# Update the viewer\n",
    "lname = f'{self_id}_consolidated'\n",
    "with viewer2.txn() as s:\n",
    "    s.layers[lname] = neuroglancer.SegmentationLayer(\n",
    "        source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "        selected_alpha=0.5)\n",
    "    # Assign segments using `set()`, since `.segments` is a Neuroglancer `VisibleSegments` object\n",
    "    s.layers[lname].segments = set(segs)\n",
    "    # s.layers[lname].segment_colors = segment_color_mapping\n",
    "    s.layers[lname].visible = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['286781112']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove keys using dictionary comprehension\n",
    "proofread_objects = {k: v for k, v in proofread_objects.items() if k not in dup_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "proofread_objects[self_id] = dup_dict_consolidated[self_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dup_dict_consolidated\n",
    "\n",
    "segs = proofread_objects[self_id]\n",
    "# color_value = generate_random_hex_color()# \"#ef8200\"\n",
    "# segment_color_mapping = {key: color_value for key in segs}\n",
    "\n",
    "\n",
    "# Update the viewer\n",
    "lname = f'{self_id}_consolidated_final'\n",
    "with viewer2.txn() as s:\n",
    "    s.layers[lname] = neuroglancer.SegmentationLayer(\n",
    "        source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "        selected_alpha=0.5)\n",
    "    # Assign segments using `set()`, since `.segments` is a Neuroglancer `VisibleSegments` object\n",
    "    s.layers[lname].segments = set(segs)\n",
    "    # s.layers[lname].segment_colors = segment_color_mapping\n",
    "    s.layers[lname].visible = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save proofread_objects if corrected now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename_proofread_list, 'w') as outfile:\n",
    "    # json.dump([{k:v} for k,v in proofread_objects.items()], outfile) #use this if want a list of dictionaries rather than a dictionary of all\n",
    "    json.dump(proofread_objects, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine dicts with same keys (or re-key them)\n",
    "\n",
    "only duplicates where overlap is not in the base segment merge list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_random_hex_color():\n",
    "  \"\"\"Generates a random hex color string.\"\"\"\n",
    "  hex_color = '#' + ''.join([random.choice('0123456789abcdef') for j in range(6)])\n",
    "  return hex_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#2363ea\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "random_color = generate_random_hex_color()\n",
    "print(random_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename_proofread_list, 'r') as f:\n",
    "    proofread_objects = json.load(f)\n",
    "    # proofread_objects = {k: v for l in proofread_objects for k, v in l.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = [k for l in proofread_objects for k, v in l.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = [item for item, count in Counter(key_list).items() if count > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['371523882', '285637634', '372669442', '371507724', '371523648']"
      ]
     },
     "execution_count": 1042,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_dict = {}\n",
    "for d in duplicates:\n",
    "    v_list = []\n",
    "    for i,l in enumerate(proofread_objects):\n",
    "        k = list(l.keys())[0]\n",
    "        if k == d:\n",
    "            v_list.append(l[k])\n",
    "    dup_dict[d] = [num for ni in v_list for num in ni]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://127.0.0.1:58455/v/9d16af3d4b925d1014b2a86b150b63f39a39731e/\" target=\"_blank\">Viewer</a>"
      ],
      "text/plain": [
       "http://127.0.0.1:58455/v/9d16af3d4b925d1014b2a86b150b63f39a39731e/"
      ]
     },
     "execution_count": 1009,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = neuroglancer.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    s.layers['image'] = neuroglancer.ImageLayer(source='brainmaps://10393113184:ell:roi450um_xyz')\n",
    "    s.layers['segmentation'] = neuroglancer.SegmentationLayer(\n",
    "        source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "        selected_alpha=0.5\n",
    "    )\n",
    "\n",
    "# Initialize a neurites counter\n",
    "viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [],
   "source": [
    "element = list(dup_dict.keys())[5]\n",
    "segs = [int(v) for v in dup_dict[element]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this element is 371524256\n",
      "Remaining neurites 3579\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define color mapping for the segment(s)\n",
    "color_value = \"#ef8200\"\n",
    "segment_color_mapping = {key: generate_random_hex_color() for key in segs}\n",
    "\n",
    "# Update the viewer\n",
    "with viewer.txn() as s:\n",
    "    # Assign segments using `set()`, since `.segments` is a Neuroglancer `VisibleSegments` object\n",
    "    s.layers['segmentation'].segments = set(segs)\n",
    "    s.layers['segmentation'].segment_colors = segment_color_mapping\n",
    "    s.layers['segmentation'].visible = True\n",
    "\n",
    "print(f'this element is {element}')\n",
    "print('Remaining neurites '+ str(len(neurites)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [],
   "source": [
    "proofread_list_og = [item for dictionary in proofread_objects for sublist in dictionary.values() for item in sublist]\n",
    "proofread_list_kp = set([num for k,v in proofread_objects_dict.items() for num in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 1054,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(proofread_list_og).difference(set(proofread_list_kp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dictionary of proofread objects from json list\n",
    "proofread_objects_dict = {k: v for l in proofread_objects for k, v in l.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each duplicate key, replace values with full set from combined dup_dict\n",
    "for k,v in dup_dict.items():\n",
    "    proofread_objects_dict[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 995,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([k for l in proofread_objects for k, v in l.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1 original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE #\n",
    "\n",
    "filename_ref_list = \"efish_box1_segments_list.csv\"\n",
    "filename_proofread_list = 'box1_split_corrected.json'\n",
    "\n",
    "# Grab the reference list segments (all segments in the box >1000 voxels)\n",
    "with open(filename_ref_list, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    ref_list = [int(value) for row in reader for value in row]  # Flattened list of uint64 values\n",
    "\n",
    "# Grab the split-corrected segments so far\n",
    "if os.path.exists(filename_proofread_list):\n",
    "    with open(filename_proofread_list, 'r') as f:\n",
    "        proofread_objects = json.load(f)\n",
    "else:\n",
    "    with open(filename_proofread_list, 'w') as outfile:  \n",
    "        json.dump([], outfile)\n",
    "    proofread_objects = []\n",
    "\n",
    "# Extract proofread segment IDs (make it a flat list) and subtract it from the remaining list\n",
    "proofread_list = [item for dictionary in proofread_objects for sublist in dictionary.values() for item in sublist]\n",
    "remaining_list = [seg_id for seg_id in ref_list if seg_id not in proofread_list]\n",
    "neurites = remaining_list\n",
    "ntotal = len(neurites)\n",
    "print('Remaining neurites '+ str(len(neurites)))\n",
    "\n",
    "viewer = neuroglancer.Viewer()\n",
    "with viewer.txn() as s:\n",
    "    s.layers['image'] = neuroglancer.ImageLayer(source='brainmaps://10393113184:ell:roi450um_xyz')\n",
    "    s.layers['segmentation'] = neuroglancer.SegmentationLayer(\n",
    "        source='brainmaps://10393113184:ell:roi450um_seg32fb16fb_220930', \n",
    "        selected_alpha=0.5\n",
    "    )\n",
    "\n",
    "# Initialize a neurites counter\n",
    "viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP2 original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining neurites 3579\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################\n",
    "# SAVE\n",
    "# print the split-corrected segments\n",
    "segments = list((viewer.state.layers['segmentation'].segments))\n",
    "for i in range( len(segments)):\n",
    "    segments[i] = int(segments[i])\n",
    "       \n",
    "\n",
    "dict_seg = {str(segments[0]): segments}\n",
    "\n",
    "# save the selected segments\n",
    "proofread_objects.append(dict_seg)\n",
    "with open(filename_proofread_list, 'w') as outfile:  \n",
    "        json.dump(proofread_objects, outfile)\n",
    "\n",
    "################################################################################################\n",
    "# RESET \n",
    "# Grab the reference list segments (all segments in the box >1000 voxels)\n",
    "with open(filename_ref_list, 'r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    ref_list = [int(value) for row in reader for value in row]  # Flattened list of uint64 values\n",
    "\n",
    "# Grab the split-corrected segments so far\n",
    "if os.path.exists(filename_proofread_list):\n",
    "    with open(filename_proofread_list, 'r') as f:\n",
    "        proofread_objects = json.load(f)\n",
    "else:\n",
    "    with open(filename_proofread_list, 'w') as outfile:  \n",
    "        json.dump([], outfile)\n",
    "    proofread_objects = []\n",
    "\n",
    "# Extract proofread segment IDs (make it a flat list) and subtract it from the remaining list\n",
    "proofread_list = [item for dictionary in proofread_objects for sublist in dictionary.values() for item in sublist]\n",
    "remaining_list = [seg_id for seg_id in ref_list if seg_id not in proofread_list]\n",
    "neurites = remaining_list\n",
    "ntotal = len(neurites)\n",
    "print('Remaining neurites '+ str(len(neurites)))\n",
    "\n",
    "# Remove all segments from the \"segmentation\" layer\n",
    "with viewer.txn() as txn:\n",
    "    layer = txn.layers['segmentation']\n",
    "    layer.segments = set()  # Assign an empty set to clear the segments\n",
    "\n",
    "with viewer.txn(overwrite=True) as s:\n",
    "    s.layers['Base Segment Merger'] = neuroglancer.AnnotationLayer()\n",
    "    s.layers['Base Segment Merger'].filterBySegmentation = [\"segments\"]\n",
    "    s.layers['Base Segment Merger'].linkedSegmentationLayer = {\"segments\": 'segmentation'}\n",
    "    s.layers['Base Segment Merger'].annotationColor = '#ffa500'\n",
    "    s.layers['Base Segment Merger'].tool = \"annotatePoint\"\n",
    "    \n",
    "\n",
    "point_type = 'ends'\n",
    "with viewer.txn(overwrite=True) as s:\n",
    "    s.layers[point_type] = neuroglancer.AnnotationLayer()\n",
    "    s.layers[point_type].annotationColor = '#ffff00'\n",
    "    s.layers[point_type].tool = \"annotatePoint\"\n",
    "    s.layers[point_type].tab = 'Annotations'\n",
    "\n",
    "# RUN STEP1 again\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vx_sizes = [16,16,30]\n",
    "t = 'Base Segment Merger'\n",
    "this_type_points = []\n",
    "for x in viewer.state.layers[t].annotations:\n",
    "    if x.segments == None:\n",
    "        c = [int(y) for y in x.point]\n",
    "        print(f'Error, no segment for point {c}, for point layer {t}, correct and re-save')\n",
    "    \n",
    "    else:\n",
    "        co_ords = [float(x) for x in list(x.point)]\n",
    "        co_ords_and_id = ([co_ords[x]*vx_sizes[x] for x in range(3)])\n",
    "        co_ords_and_id.append(str(x.segments[0][0]))\n",
    "        \n",
    "        this_type_points.append((co_ords_and_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(this_type_points,columns = ['x','y','z','segID'])\n",
    "df.to_csv('Base_Segment_Merger.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Base_Segment_Merger.csv')\n",
    "base_seg_merge_points = df[['x','y','z','segID']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos,point in enumerate(base_seg_merge_points):\n",
    "    point_array = np.asarray([int(point[x]/vx_sizes[x]) for x in range(3)])\n",
    "    pa = neuroglancer.PointAnnotation(id=f'bm_{pos}', point = point_array, segments=[[point[3]]])\n",
    "    s.layers['Base Segment Merger'].annotations.append(pa)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename_proofread_list, 'r') as f:\n",
    "    proofread_objects = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dict = {}\n",
    "for i in proofread_objects:\n",
    "    for k,v in i.items():\n",
    "        split_dict[k]=v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "proofread_glia_segs_0 = set(split_dict['286796800'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "proofread_glia_segs_1 = set(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = list(proofread_glia_segs_1.union(proofread_glia_segs_0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
